{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Snow Depth Prediction\n",
    "\n",
    "In this section, we will perform feature engineering for each resort's dataset individually. Our goal is to prepare the data for modeling by creating new features and handling missing values appropriately.\n",
    "\n",
    "### Steps:\n",
    "- **Load Data**: Each cleaned resort-specific CSV file is loaded from the processed directory.\n",
    "- **Lag Features**: We create lagged features for `snow_depth` (one day and seven days prior) to capture temporal dependencies.\n",
    "- **Temperature Features**: We calculate the average temperature (`temperature_avg`) and its square (`temperature_avg_squared`) to account for potential non-linear relationships with snow depth.\n",
    "- **Missing Values Handling**: Rows with any missing data in the selected features and target variable are dropped to ensure data quality for modeling.\n",
    "- **Saving Processed Data**: Each resortâ€™s processed data is saved as a separate CSV file for streamlined access in the modeling stage.\n",
    "\n",
    "This approach ensures each resort is treated individually, maintaining flexibility for subsequent analysis and model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data root absolute path: /workspace/SkiSnow/notebooks/data/processed/cds\n",
      "Does the processed data root exist? False\n"
     ]
    }
   ],
   "source": [
    "processed_data_root = os.path.abspath(os.path.join('data', 'processed', 'cds'))\n",
    "\n",
    "print(\"Processed data root absolute path:\", processed_data_root)\n",
    "print(\"Does the processed data root exist?\", os.path.exists(processed_data_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading & combining required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 files:\n",
      "../data/processed/cds/austrian_alps/kitzbuhel/kitzbuhel_cleaned_2024-10-28_13-30-56.csv\n",
      "../data/processed/cds/austrian_alps/solden/solden_cleaned_2024-10-28_13-30-56.csv\n",
      "../data/processed/cds/austrian_alps/st_anton/st_anton_cleaned_2024-10-28_13-30-56.csv\n",
      "../data/processed/cds/italian_alps/cortina_d_ampezzo/cortina_d_ampezzo_cleaned_2024-10-28_13-30-56.csv\n",
      "../data/processed/cds/italian_alps/sestriere/sestriere_cleaned_2024-10-28_13-30-56.csv\n",
      "../data/processed/cds/italian_alps/val_gardena/val_gardena_cleaned_2024-10-28_13-30-56.csv\n",
      "../data/processed/cds/slovenian_alps/kranjska_gora/kranjska_gora_cleaned_2024-10-28_13-30-56.csv\n",
      "../data/processed/cds/slovenian_alps/krvavec/krvavec_cleaned_2024-10-28_13-30-56.csv\n",
      "../data/processed/cds/slovenian_alps/mariborsko_pohorje/mariborsko_pohorje_cleaned_2024-10-28_13-30-56.csv\n",
      "../data/processed/cds/swiss_alps/st_moritz/st_moritz_cleaned_2024-10-28_13-30-56.csv\n",
      "Combined data shape: (121351, 8)\n"
     ]
    }
   ],
   "source": [
    "processed_data_root = '../data/processed/cds'\n",
    "\n",
    "# Initialize a list to store DataFrames\n",
    "combined_data = []\n",
    "\n",
    "# Use glob to find all cleaned CSV files in the processed data directory\n",
    "csv_files = glob.glob(os.path.join(processed_data_root, '**', '*_cleaned_*.csv'), recursive=True)\n",
    "\n",
    "print(f\"Found {len(csv_files)} files:\")\n",
    "for file_path in csv_files:\n",
    "    print(file_path)\n",
    "\n",
    "for file_path in csv_files:\n",
    "    # Load each resort-specific DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract resort information from the file path\n",
    "    resort_name = os.path.basename(os.path.dirname(file_path))\n",
    "    df['resort'] = resort_name\n",
    "\n",
    "    # Append to the list\n",
    "    combined_data.append(df)\n",
    "\n",
    "# Concatenating all DataFrames into one\n",
    "model_data = pd.concat(combined_data, ignore_index=True)\n",
    "\n",
    "# Displaying the shape to confirm combining\n",
    "print(f\"Combined data shape: {model_data.shape}\")\n",
    "\n",
    "# Ensuring 'date' column is in datetime format\n",
    "model_data['date'] = pd.to_datetime(model_data['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data['snow_depth_lag1'] = model_data.groupby('resort')['snow_depth'].shift(1)\n",
    "model_data['snow_depth_lag7'] = model_data.groupby('resort')['snow_depth'].shift(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculating Temperature Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the average temperature\n",
    "model_data['temperature_avg'] = (model_data['temperature_min'] + model_data['temperature_max']) / 2\n",
    "\n",
    "# Calculating the squared average temperature to capture non-linear effects\n",
    "model_data['temperature_avg_squared'] = model_data['temperature_avg'] ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after dropping missing values: (57862, 12)\n"
     ]
    }
   ],
   "source": [
    "# Defining the features\n",
    "features = ['temperature_avg', 'temperature_avg_squared', 'precipitation_sum', 'snow_depth_lag1', 'snow_depth_lag7']\n",
    "\n",
    "# Dropping rows with any missing values in the selected features or target variable\n",
    "model_data = model_data.dropna(subset=features + ['snow_depth'])\n",
    "\n",
    "# Reseting index after dropping rows\n",
    "model_data = model_data.reset_index(drop=True)\n",
    "\n",
    "# Displaying the shape after dropping missing values\n",
    "print(f\"Data shape after dropping missing values: {model_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined processed data to ../data/processed/processed_data_for_modeling.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = os.path.join('..', 'data', 'processed', 'processed_data_for_modeling.csv')\n",
    "\n",
    "# Save the combined processed data\n",
    "model_data.to_csv(output_path, index=False)\n",
    "print(f\"Saved combined processed data to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
