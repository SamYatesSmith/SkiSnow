{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling for Snow Depth Prediction\n",
    "\n",
    "This notebook is focused on preparing the data for modeling, addressing multicollinearity, and creating training, validation, and testing sets. We will proceed with the following steps:\n",
    "- Loading the Processed Data\n",
    "- Creating Lag Features and Derived Variables\n",
    "- Handling Multicollinearity\n",
    "- Splitting the Data into Training, Validation, and Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Libraries and Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined data with shape (57862, 12)\n"
     ]
    }
   ],
   "source": [
    "# Adjust the path based on your current working directory\n",
    "data_path = os.path.join('..', 'data', 'processed', 'processed_data_for_modeling.csv')\n",
    "\n",
    "# Load the combined processed data\n",
    "model_data = pd.read_csv(data_path)\n",
    "\n",
    "# Ensure 'date' column is in datetime format\n",
    "model_data['date'] = pd.to_datetime(model_data['date'])\n",
    "\n",
    "# Display the shape to confirm loading\n",
    "print(f\"Loaded combined data with shape {model_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initiating the \"Resort\" feauture\n",
    "\n",
    "The data now includes the \"resort\" feature, we need to handle it appropriately for modeling.  Since 'resort' is a categorical variable, we need to encode it into numerical format using One-Hot encoding.  Furthermore, we'll avoid multicollinearity by dropping one of the dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after encoding: ['date', 'temperature_min', 'temperature_max', 'precipitation_sum', 'snow_depth', 'season_id', 'is_operating_season', 'snow_depth_lag1', 'snow_depth_lag7', 'temperature_avg', 'temperature_avg_squared', 'resort_cortina_d_ampezzo', 'resort_kitzbuhel', 'resort_kranjska_gora', 'resort_krvavec', 'resort_les_trois_vallees', 'resort_mariborsko_pohorje', 'resort_sestriere', 'resort_solden', 'resort_st_anton', 'resort_st_moritz', 'resort_val_d_isere_tignes', 'resort_val_gardena', 'resort_verbier']\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encode the 'resort' feature\n",
    "model_data = pd.get_dummies(model_data, columns=['resort'], drop_first=True)\n",
    "\n",
    "# Display the columns to confirm encoding\n",
    "print(f\"Columns after encoding: {model_data.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preparing Features and Target Variable\n",
    "\n",
    "Features defined include: \n",
    "- temperature_avg\n",
    "- temperature_avg_squared\n",
    "- precipitation_sum\n",
    "- snow_depth_lag1\n",
    "- snow_depth_lag7\n",
    "- encoded resort\n",
    "\n",
    "Defining the target variable - snow_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after filtering for operating season: (39668, 24)\n",
      "Initial shape of X (including 'snow_depth'): (39668, 23)\n",
      "Initial shape of y: (39668,)\n"
     ]
    }
   ],
   "source": [
    "data_operating = model_data[model_data['is_operating_season'] == True].reset_index(drop=True)\n",
    "\n",
    "print(f\"Data shape after filtering for operating season: {data_operating.shape}\")\n",
    "\n",
    "# Define the target variable\n",
    "y = data_operating['snow_depth']\n",
    "\n",
    "# Exclude 'date' and 'snow_depth' from features\n",
    "feature_columns = [col for col in data_operating.columns if col != 'date']\n",
    "\n",
    "# Create the features DataFrame\n",
    "X = data_operating[feature_columns].copy()\n",
    "\n",
    "print(f\"Initial shape of X (including 'snow_depth'): {X.shape}\")\n",
    "print(f\"Initial shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Check for Non-Numeric Columns & Handle \"season_id\"\n",
    "\n",
    "Ensure any columns in x that are of data types, object or bool.  Non numeric values need to be handled before modeling.  Subsequently perform one-hot encoding to convert season_id into numeric dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns before processing: ['season_id', 'is_operating_season', 'resort_cortina_d_ampezzo', 'resort_kitzbuhel', 'resort_kranjska_gora', 'resort_krvavec', 'resort_les_trois_vallees', 'resort_mariborsko_pohorje', 'resort_sestriere', 'resort_solden', 'resort_st_anton', 'resort_st_moritz', 'resort_val_d_isere_tignes', 'resort_val_gardena', 'resort_verbier']\n",
      "One-Hot Encoded 'season_id' and updated X.\n",
      "Converted boolean columns to integers: ['is_operating_season', 'resort_cortina_d_ampezzo', 'resort_kitzbuhel', 'resort_kranjska_gora', 'resort_krvavec', 'resort_les_trois_vallees', 'resort_mariborsko_pohorje', 'resort_sestriere', 'resort_solden', 'resort_st_anton', 'resort_st_moritz', 'resort_val_d_isere_tignes', 'resort_val_gardena', 'resort_verbier', 'season_1991-1992', 'season_1992-1993', 'season_1993-1994', 'season_1994-1995', 'season_1995-1996', 'season_1996-1997', 'season_1997-1998', 'season_1998-1999', 'season_1999-2000', 'season_2000-2001', 'season_2001-2002', 'season_2002-2003', 'season_2003-2004', 'season_2004-2005', 'season_2005-2006', 'season_2006-2007', 'season_2007-2008', 'season_2008-2009', 'season_2009-2010', 'season_2010-2011', 'season_2011-2012', 'season_2012-2013', 'season_2013-2014', 'season_2014-2015', 'season_2015-2016', 'season_2016-2017', 'season_2017-2018', 'season_2018-2019', 'season_2019-2020', 'season_2020-2021', 'season_2021-2022', 'season_2022-2023', 'season_2023-2024']\n",
      "Non-numeric columns after processing: []\n",
      "Data types after processing:\n",
      "temperature_min              float64\n",
      "temperature_max              float64\n",
      "precipitation_sum            float64\n",
      "snow_depth                   float64\n",
      "is_operating_season            int64\n",
      "snow_depth_lag1              float64\n",
      "snow_depth_lag7              float64\n",
      "temperature_avg              float64\n",
      "temperature_avg_squared      float64\n",
      "resort_cortina_d_ampezzo       int64\n",
      "resort_kitzbuhel               int64\n",
      "resort_kranjska_gora           int64\n",
      "resort_krvavec                 int64\n",
      "resort_les_trois_vallees       int64\n",
      "resort_mariborsko_pohorje      int64\n",
      "resort_sestriere               int64\n",
      "resort_solden                  int64\n",
      "resort_st_anton                int64\n",
      "resort_st_moritz               int64\n",
      "resort_val_d_isere_tignes      int64\n",
      "resort_val_gardena             int64\n",
      "resort_verbier                 int64\n",
      "season_1991-1992               int64\n",
      "season_1992-1993               int64\n",
      "season_1993-1994               int64\n",
      "season_1994-1995               int64\n",
      "season_1995-1996               int64\n",
      "season_1996-1997               int64\n",
      "season_1997-1998               int64\n",
      "season_1998-1999               int64\n",
      "season_1999-2000               int64\n",
      "season_2000-2001               int64\n",
      "season_2001-2002               int64\n",
      "season_2002-2003               int64\n",
      "season_2003-2004               int64\n",
      "season_2004-2005               int64\n",
      "season_2005-2006               int64\n",
      "season_2006-2007               int64\n",
      "season_2007-2008               int64\n",
      "season_2008-2009               int64\n",
      "season_2009-2010               int64\n",
      "season_2010-2011               int64\n",
      "season_2011-2012               int64\n",
      "season_2012-2013               int64\n",
      "season_2013-2014               int64\n",
      "season_2014-2015               int64\n",
      "season_2015-2016               int64\n",
      "season_2016-2017               int64\n",
      "season_2017-2018               int64\n",
      "season_2018-2019               int64\n",
      "season_2019-2020               int64\n",
      "season_2020-2021               int64\n",
      "season_2021-2022               int64\n",
      "season_2022-2023               int64\n",
      "season_2023-2024               int64\n",
      "dtype: object\n",
      "All features are now numeric and ready for Polynomial Features.\n"
     ]
    }
   ],
   "source": [
    "# Check for non-numeric columns\n",
    "non_numeric_cols = X.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "print(f\"Non-numeric columns before processing: {non_numeric_cols}\")\n",
    "\n",
    "# Handle 'season_id' column\n",
    "# One-Hot Encode 'season_id' if it's in the features\n",
    "if 'season_id' in X.columns:\n",
    "    # One-Hot Encode 'season_id'\n",
    "    season_dummies = pd.get_dummies(X['season_id'], prefix='season', drop_first=True)\n",
    "    # Drop 'season_id' and add the encoded variables\n",
    "    X = pd.concat([X.drop(columns=['season_id']), season_dummies], axis=1)\n",
    "    print(\"One-Hot Encoded 'season_id' and updated X.\")\n",
    "else:\n",
    "    print(\"'season_id' column not found in features.\")\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "bool_columns = X.select_dtypes(include=['bool']).columns.tolist()\n",
    "if bool_columns:\n",
    "    X[bool_columns] = X[bool_columns].astype(int)\n",
    "    print(f\"Converted boolean columns to integers: {bool_columns}\")\n",
    "\n",
    "# Verify all columns are now numeric\n",
    "non_numeric_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Non-numeric columns after processing: {non_numeric_cols}\")\n",
    "print(\"Data types after processing:\")\n",
    "print(X.dtypes)\n",
    "\n",
    "if not non_numeric_cols:\n",
    "    print(\"All features are now numeric and ready for Polynomial Features.\")\n",
    "else:\n",
    "    print(\"There are still non-numeric features that need to be encoded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Lag Features\n",
    "\n",
    "Incorporate lagged values of snow depth and other relevant features to capture temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created lag feature 'snow_depth_lag1'.\n",
      "Created lag feature 'snow_depth_lag7'.\n",
      "Created lag feature 'snow_depth_lag14'.\n",
      "Created lag feature 'snow_depth_lag21'.\n",
      "Created lag feature 'temperature_avg_lag1'.\n",
      "Created lag feature 'temperature_avg_lag7'.\n",
      "Created lag feature 'temperature_avg_lag14'.\n",
      "Created lag feature 'temperature_avg_lag21'.\n",
      "Created lag feature 'precipitation_sum_lag1'.\n",
      "Created lag feature 'precipitation_sum_lag7'.\n",
      "Created lag feature 'precipitation_sum_lag14'.\n",
      "Created lag feature 'precipitation_sum_lag21'.\n",
      "Shape of X after adding lag features: (39647, 65)\n",
      "Shape of y after adding lag features: (39647,)\n"
     ]
    }
   ],
   "source": [
    "# Define lag periods\n",
    "lags = [1, 7, 14, 21]  # You can adjust these based on domain knowledge\n",
    "\n",
    "# Features to create lag features for\n",
    "lag_features = ['snow_depth', 'temperature_avg', 'precipitation_sum']\n",
    "\n",
    "for feature in lag_features:\n",
    "    for lag in lags:\n",
    "        lag_col = f\"{feature}_lag{lag}\"\n",
    "        if feature in X.columns:\n",
    "            X[lag_col] = X[feature].shift(lag)\n",
    "            print(f\"Created lag feature '{lag_col}'.\")\n",
    "        else:\n",
    "            print(f\"Feature '{feature}' not found in X. Skipping lag creation for this feature.\")\n",
    "\n",
    "# Drop rows with NaN values resulting from lagging\n",
    "# Ensure that any rows dropped are reflected in both X and y\n",
    "initial_length = len(X)\n",
    "X = X.dropna().reset_index(drop=True)\n",
    "y = y.iloc[X.index].reset_index(drop=True)\n",
    "\n",
    "print(f\"Shape of X after adding lag features: {X.shape}\")\n",
    "print(f\"Shape of y after adding lag features: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Handling Multicollinearity\n",
    "\n",
    "Before splitting the data, it's important to check for multicollinearity among features.\n",
    "\n",
    "Calculating the Variance Inflation Factor (VIF) for each feature is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/SkiSnow/venv/lib/python3.12/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Variance Inflation Factors:\n",
      "                    feature        VIF\n",
      "0           temperature_min        inf\n",
      "1           temperature_max        inf\n",
      "6           temperature_avg        inf\n",
      "3       is_operating_season  73.420689\n",
      "56     temperature_avg_lag1   5.537389\n",
      "..                      ...        ...\n",
      "53         season_2023-2024   1.268330\n",
      "7   temperature_avg_squared   1.239701\n",
      "61   precipitation_sum_lag7   1.218813\n",
      "62  precipitation_sum_lag14   1.210810\n",
      "63  precipitation_sum_lag21   1.187878\n",
      "\n",
      "[64 rows x 2 columns]\n",
      "Features with VIF > 10: ['temperature_min', 'temperature_max', 'is_operating_season', 'temperature_avg']\n",
      "Dropped feature 'temperature_min' due to high VIF.\n",
      "                    feature        VIF\n",
      "2       is_operating_season  73.420689\n",
      "5           temperature_avg  21.341858\n",
      "0           temperature_max  17.439587\n",
      "55     temperature_avg_lag1   5.537389\n",
      "19           resort_verbier   4.779857\n",
      "..                      ...        ...\n",
      "52         season_2023-2024   1.268330\n",
      "6   temperature_avg_squared   1.239701\n",
      "60   precipitation_sum_lag7   1.218813\n",
      "61  precipitation_sum_lag14   1.210810\n",
      "62  precipitation_sum_lag21   1.187878\n",
      "\n",
      "[63 rows x 2 columns]\n",
      "Dropped feature 'temperature_max' due to high VIF.\n",
      "                    feature        VIF\n",
      "1       is_operating_season  71.564148\n",
      "54     temperature_avg_lag1   5.488337\n",
      "4           temperature_avg   5.085692\n",
      "18           resort_verbier   4.671043\n",
      "7          resort_kitzbuhel   4.029748\n",
      "..                      ...        ...\n",
      "51         season_2023-2024   1.267652\n",
      "59   precipitation_sum_lag7   1.218369\n",
      "60  precipitation_sum_lag14   1.210659\n",
      "61  precipitation_sum_lag21   1.187620\n",
      "5   temperature_avg_squared   1.116318\n",
      "\n",
      "[62 rows x 2 columns]\n",
      "Dropped feature 'is_operating_season' due to high VIF.\n",
      "                     feature       VIF\n",
      "2            snow_depth_lag7  5.838331\n",
      "51          snow_depth_lag14  5.664006\n",
      "53      temperature_avg_lag1  5.557901\n",
      "3            temperature_avg  5.148351\n",
      "1            snow_depth_lag1  5.124061\n",
      "..                       ...       ...\n",
      "59   precipitation_sum_lag14  1.366952\n",
      "16        resort_val_gardena  1.360176\n",
      "5   resort_cortina_d_ampezzo  1.351981\n",
      "60   precipitation_sum_lag21  1.341907\n",
      "50          season_2023-2024  1.130080\n",
      "\n",
      "[61 rows x 2 columns]\n",
      "Dropped feature 'temperature_avg' due to high VIF.\n",
      "                      feature       VIF\n",
      "2             snow_depth_lag7  5.837981\n",
      "50           snow_depth_lag14  5.663952\n",
      "1             snow_depth_lag1  5.123502\n",
      "51           snow_depth_lag21  4.772798\n",
      "16             resort_verbier  3.367964\n",
      "5            resort_kitzbuhel  3.076499\n",
      "12            resort_st_anton  2.723073\n",
      "6        resort_kranjska_gora  2.610777\n",
      "53       temperature_avg_lag7  2.420261\n",
      "54      temperature_avg_lag14  2.337657\n",
      "11              resort_solden  2.295380\n",
      "9   resort_mariborsko_pohorje  2.238007\n",
      "52       temperature_avg_lag1  2.205244\n",
      "55      temperature_avg_lag21  2.096369\n",
      "7              resort_krvavec  1.989943\n",
      "10           resort_sestriere  1.953089\n",
      "13           resort_st_moritz  1.787388\n",
      "35           season_2009-2010  1.781295\n",
      "14  resort_val_d_isere_tignes  1.765796\n",
      "34           season_2008-2009  1.765211\n",
      "37           season_2011-2012  1.762964\n",
      "38           season_2012-2013  1.706229\n",
      "39           season_2013-2014  1.689406\n",
      "17           season_1991-1992  1.657856\n",
      "43           season_2017-2018  1.651355\n",
      "36           season_2010-2011  1.647043\n",
      "3     temperature_avg_squared  1.646957\n",
      "40           season_2014-2015  1.618697\n",
      "33           season_2007-2008  1.606277\n",
      "8    resort_les_trois_vallees  1.594712\n",
      "44           season_2018-2019  1.594474\n",
      "48           season_2022-2023  1.560127\n",
      "25           season_1999-2000  1.555168\n",
      "28           season_2002-2003  1.551095\n",
      "24           season_1998-1999  1.542536\n",
      "56     precipitation_sum_lag1  1.539155\n",
      "46           season_2020-2021  1.535710\n",
      "41           season_2015-2016  1.533743\n",
      "47           season_2021-2022  1.532141\n",
      "21           season_1995-1996  1.518930\n",
      "29           season_2003-2004  1.509122\n",
      "45           season_2019-2020  1.508856\n",
      "0           precipitation_sum  1.505187\n",
      "32           season_2006-2007  1.492948\n",
      "42           season_2016-2017  1.490560\n",
      "19           season_1993-1994  1.487914\n",
      "26           season_2000-2001  1.480314\n",
      "20           season_1994-1995  1.478722\n",
      "27           season_2001-2002  1.469401\n",
      "31           season_2005-2006  1.459218\n",
      "18           season_1992-1993  1.452041\n",
      "22           season_1996-1997  1.447989\n",
      "30           season_2004-2005  1.431494\n",
      "23           season_1997-1998  1.416573\n",
      "57     precipitation_sum_lag7  1.374361\n",
      "58    precipitation_sum_lag14  1.366885\n",
      "15         resort_val_gardena  1.359838\n",
      "4    resort_cortina_d_ampezzo  1.351728\n",
      "59    precipitation_sum_lag21  1.341886\n",
      "49           season_2023-2024  1.129965\n",
      "Final Variance Inflation Factors:\n",
      "                      feature       VIF\n",
      "2             snow_depth_lag7  5.837981\n",
      "50           snow_depth_lag14  5.663952\n",
      "1             snow_depth_lag1  5.123502\n",
      "51           snow_depth_lag21  4.772798\n",
      "16             resort_verbier  3.367964\n",
      "5            resort_kitzbuhel  3.076499\n",
      "12            resort_st_anton  2.723073\n",
      "6        resort_kranjska_gora  2.610777\n",
      "53       temperature_avg_lag7  2.420261\n",
      "54      temperature_avg_lag14  2.337657\n",
      "11              resort_solden  2.295380\n",
      "9   resort_mariborsko_pohorje  2.238007\n",
      "52       temperature_avg_lag1  2.205244\n",
      "55      temperature_avg_lag21  2.096369\n",
      "7              resort_krvavec  1.989943\n",
      "10           resort_sestriere  1.953089\n",
      "13           resort_st_moritz  1.787388\n",
      "35           season_2009-2010  1.781295\n",
      "14  resort_val_d_isere_tignes  1.765796\n",
      "34           season_2008-2009  1.765211\n",
      "37           season_2011-2012  1.762964\n",
      "38           season_2012-2013  1.706229\n",
      "39           season_2013-2014  1.689406\n",
      "17           season_1991-1992  1.657856\n",
      "43           season_2017-2018  1.651355\n",
      "36           season_2010-2011  1.647043\n",
      "3     temperature_avg_squared  1.646957\n",
      "40           season_2014-2015  1.618697\n",
      "33           season_2007-2008  1.606277\n",
      "8    resort_les_trois_vallees  1.594712\n",
      "44           season_2018-2019  1.594474\n",
      "48           season_2022-2023  1.560127\n",
      "25           season_1999-2000  1.555168\n",
      "28           season_2002-2003  1.551095\n",
      "24           season_1998-1999  1.542536\n",
      "56     precipitation_sum_lag1  1.539155\n",
      "46           season_2020-2021  1.535710\n",
      "41           season_2015-2016  1.533743\n",
      "47           season_2021-2022  1.532141\n",
      "21           season_1995-1996  1.518930\n",
      "29           season_2003-2004  1.509122\n",
      "45           season_2019-2020  1.508856\n",
      "0           precipitation_sum  1.505187\n",
      "32           season_2006-2007  1.492948\n",
      "42           season_2016-2017  1.490560\n",
      "19           season_1993-1994  1.487914\n",
      "26           season_2000-2001  1.480314\n",
      "20           season_1994-1995  1.478722\n",
      "27           season_2001-2002  1.469401\n",
      "31           season_2005-2006  1.459218\n",
      "18           season_1992-1993  1.452041\n",
      "22           season_1996-1997  1.447989\n",
      "30           season_2004-2005  1.431494\n",
      "23           season_1997-1998  1.416573\n",
      "57     precipitation_sum_lag7  1.374361\n",
      "58    precipitation_sum_lag14  1.366885\n",
      "15         resort_val_gardena  1.359838\n",
      "4    resort_cortina_d_ampezzo  1.351728\n",
      "59    precipitation_sum_lag21  1.341886\n",
      "49           season_2023-2024  1.129965\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate VIF\n",
    "def calculate_vif(X):\n",
    "    vif = pd.DataFrame()\n",
    "    vif['feature'] = X.columns\n",
    "    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif\n",
    "\n",
    "# Exclude 'snow_depth' from VIF calculation if present\n",
    "features_to_exclude = ['snow_depth']\n",
    "X_vif = X.drop(columns=features_to_exclude, errors='ignore').copy()\n",
    "\n",
    "# Calculate initial VIF\n",
    "vif_data = calculate_vif(X_vif)\n",
    "print(\"Initial Variance Inflation Factors:\")\n",
    "print(vif_data.sort_values('VIF', ascending=False))\n",
    "\n",
    "# Identify features with VIF > 10\n",
    "high_vif = vif_data[vif_data['VIF'] > 10]['feature'].tolist()\n",
    "print(f\"Features with VIF > 10: {high_vif}\")\n",
    "\n",
    "# Iteratively remove features with high VIF\n",
    "for feature in high_vif:\n",
    "    if feature in X.columns:\n",
    "        X = X.drop(columns=[feature])\n",
    "        print(f\"Dropped feature '{feature}' due to high VIF.\")\n",
    "        # Recalculate VIF after dropping the feature\n",
    "        X_vif = X.drop(columns=features_to_exclude, errors='ignore').copy()\n",
    "        vif_data = calculate_vif(X_vif)\n",
    "        print(vif_data.sort_values('VIF', ascending=False))\n",
    "    else:\n",
    "        print(f\"Feature '{feature}' already removed.\")\n",
    "\n",
    "# Final VIF after removal\n",
    "final_vif = calculate_vif(X_vif)\n",
    "print(\"Final Variance Inflation Factors:\")\n",
    "print(final_vif.sort_values('VIF', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Adding Polynomial Features\n",
    "\n",
    "Now that all non-numeric columns have been handled, we can safely apply Polynomial Features to capture non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot apply PolynomialFeatures to missing features: ['temperature_avg']\n",
      "Proceeding with available features only.\n",
      "Shape of X after adding polynomial features to selected features: (39647, 62)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define features to apply PolynomialFeatures\n",
    "selected_features = ['temperature_avg', 'precipitation_sum']  # Adjust based on domain knowledge\n",
    "\n",
    "# Check which selected features are present in X\n",
    "available_features = [feature for feature in selected_features if feature in X.columns]\n",
    "missing_features = [feature for feature in selected_features if feature not in X.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"Cannot apply PolynomialFeatures to missing features: {missing_features}\")\n",
    "    print(\"Proceeding with available features only.\")\n",
    "else:\n",
    "    print(\"All selected features are available for PolynomialFeatures.\")\n",
    "\n",
    "if available_features:\n",
    "    # Initialize PolynomialFeatures with degree 2\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    \n",
    "    # Fit and transform only the available features\n",
    "    X_poly_selected = poly.fit_transform(X[available_features])\n",
    "    \n",
    "    # Get the new feature names for selected features\n",
    "    poly_features_selected = poly.get_feature_names_out(available_features)\n",
    "    \n",
    "    # Create a DataFrame for polynomial features\n",
    "    X_poly_df = pd.DataFrame(X_poly_selected, columns=poly_features_selected)\n",
    "    \n",
    "    # Drop the original selected features from X\n",
    "    X = X.drop(columns=available_features)\n",
    "    \n",
    "    # Concatenate the polynomial features back to X\n",
    "    X = pd.concat([X.reset_index(drop=True), X_poly_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    print(f\"Shape of X after adding polynomial features to selected features: {X.shape}\")\n",
    "else:\n",
    "    print(\"No features available for PolynomialFeatures. Skipping this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Splitting the Data\n",
    "\n",
    "Since the data is time-series data, it's important to split it in a way that respects the temporal order tand thus avoid data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9 (a) Split the data into training, validation, and test sets using time-based splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (27752, 62), y_train size: (27752,)\n",
      "Validation set size: (5947, 62), y_val size: (5947,)\n",
      "Test set size: (5948, 62), y_test size: (5948,)\n",
      "All splits are aligned correctly.\n"
     ]
    }
   ],
   "source": [
    "# Define the sizes for training, validation, and testing sets\n",
    "total_length = len(X)\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "test_size = total_length - train_size - val_size\n",
    "\n",
    "# Split the data\n",
    "X_train = X.iloc[:train_size].reset_index(drop=True)\n",
    "y_train = y.iloc[:train_size].reset_index(drop=True)\n",
    "\n",
    "X_val = X.iloc[train_size:train_size + val_size].reset_index(drop=True)\n",
    "y_val = y.iloc[train_size:train_size + val_size].reset_index(drop=True)\n",
    "\n",
    "X_test = X.iloc[train_size + val_size:].reset_index(drop=True)\n",
    "y_test = y.iloc[train_size + val_size:].reset_index(drop=True)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}, y_train size: {y_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}, y_val size: {y_val.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}, y_test size: {y_test.shape}\")\n",
    "\n",
    "# Verify alignment\n",
    "assert X_train.shape[0] == y_train.shape[0], \"Mismatch in training set.\"\n",
    "assert X_val.shape[0] == y_val.shape[0], \"Mismatch in validation set.\"\n",
    "assert X_test.shape[0] == y_test.shape[0], \"Mismatch in test set.\"\n",
    "\n",
    "print(\"All splits are aligned correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Proceeding with Modeling\n",
    "\n",
    "The data is now prepared.  We can proceed to build and evaluate the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 (a) Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "snow_depth                  float64\n",
      "snow_depth_lag1             float64\n",
      "snow_depth_lag7             float64\n",
      "temperature_avg_squared     float64\n",
      "resort_cortina_d_ampezzo      int64\n",
      "                             ...   \n",
      "precipitation_sum_lag7      float64\n",
      "precipitation_sum_lag14     float64\n",
      "precipitation_sum_lag21     float64\n",
      "precipitation_sum           float64\n",
      "precipitation_sum^2         float64\n",
      "Length: 62, dtype: object\n",
      "Non-numeric columns in X_train: []\n"
     ]
    }
   ],
   "source": [
    "print('season_id' in X_train.columns)\n",
    "print(X_train.dtypes)\n",
    "\n",
    "non_numeric_cols_train = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Non-numeric columns in X_train: {non_numeric_cols_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters found: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}\n",
      "\n",
      "Optimised Random Forest Regressor Evaluation on Validation Set:\n",
      "Mean Squared Error: 1173.11\n",
      "R-squared: 0.94\n",
      "\n",
      "Length of y_test: 5948\n",
      "Length of y_test_pred: 5948\n",
      "\n",
      "Optimised Random Forest Regressor Evaluation on Test Set:\n",
      "Mean Squared Error: 1452.32\n",
      "R-squared: 0.93\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Initialize Grid Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters found:\", random_search.best_params_)\n",
    "\n",
    "# Use the best estimator\n",
    "best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# Predict and evaluate\n",
    "y_val_pred = best_rf_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"\\nOptimised Random Forest Regressor Evaluation on Validation Set:\")\n",
    "print(f\"Mean Squared Error: {mse_val:.2f}\")\n",
    "print(f\"R-squared: {r2_val:.2f}\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(f\"\\nLength of y_test: {len(y_test)}\")\n",
    "print(f\"Length of y_test_pred: {len(y_test_pred)}\")\n",
    "\n",
    "# Verify alignment before evaluation\n",
    "assert len(y_test) == len(y_test_pred), \"Mismatch in number of samples between y_test and y_test_pred.\"\n",
    "\n",
    "# Evaluate the model\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nOptimised Random Forest Regressor Evaluation on Test Set:\")\n",
    "print(f\"Mean Squared Error: {mse_test:.2f}\")\n",
    "print(f\"R-squared: {r2_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model saved to ../src/models/best_rf_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the model\n",
    "model_save_dir = os.path.join('..', 'src', 'models')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Define the path to save the model\n",
    "model_save_path = os.path.join(model_save_dir, 'best_rf_model.pkl')\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(best_rf_model, model_save_path)\n",
    "print(f\"Trained model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Saving the Split Datasets\n",
    "\n",
    "After training and evaluating the model, we'll save the split datasets (`X_train.csv`, `y_train.csv`, `X_val.csv`, `y_val.csv`, `X_test.csv`, `y_test.csv`) to the `data/processed/modeling_data/` directory. This ensures that these datasets can be easily accessed for future evaluations, residual analysis, and other analyses without the need to regenerate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved split datasets to ../data/processed/modeling_data\n"
     ]
    }
   ],
   "source": [
    "# Define the directory to save the split datasets\n",
    "modeling_data_dir = os.path.join('..', 'data', 'processed', 'modeling_data')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(modeling_data_dir, exist_ok=True)\n",
    "\n",
    "# Save the split datasets\n",
    "X_train.to_csv(os.path.join(modeling_data_dir, 'X_train.csv'), index=False)\n",
    "y_train.to_csv(os.path.join(modeling_data_dir, 'y_train.csv'), index=False)\n",
    "\n",
    "X_val.to_csv(os.path.join(modeling_data_dir, 'X_val.csv'), index=False)\n",
    "y_val.to_csv(os.path.join(modeling_data_dir, 'y_val.csv'), index=False)\n",
    "\n",
    "X_test.to_csv(os.path.join(modeling_data_dir, 'X_test.csv'), index=False)\n",
    "y_test.to_csv(os.path.join(modeling_data_dir, 'y_test.csv'), index=False)\n",
    "\n",
    "print(f\"Saved split datasets to {modeling_data_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
