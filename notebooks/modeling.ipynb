{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling for Snow Depth Prediction\n",
    "\n",
    "This notebook is focused on preparing the data for modeling, addressing multicollinearity, and creating training, validation, and testing sets. We will proceed with the following steps:\n",
    "- Loading the Processed Data\n",
    "- Creating Lag Features and Derived Variables\n",
    "- Handling Multicollinearity\n",
    "- Splitting the Data into Training, Validation, and Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Libraries and Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded combined data with shape (57862, 12)\n"
     ]
    }
   ],
   "source": [
    "# Adjust the path based on your current working directory\n",
    "data_path = os.path.join('..', 'data', 'processed', 'processed_data_for_modeling.csv')\n",
    "\n",
    "# Load the combined processed data\n",
    "model_data = pd.read_csv(data_path)\n",
    "\n",
    "# Ensure 'date' column is in datetime format\n",
    "model_data['date'] = pd.to_datetime(model_data['date'])\n",
    "\n",
    "# Display the shape to confirm loading\n",
    "print(f\"Loaded combined data with shape {model_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initiating the \"Resort\" feauture\n",
    "\n",
    "The data now includes the \"resort\" feature, we need to handle it appropriately for modeling.  Since 'resort' is a categorical variable, we need to encode it into numerical format using One-Hot encoding.  Furthermore, we'll avoid multicollinearity by dropping one of the dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after encoding: ['date', 'temperature_min', 'temperature_max', 'precipitation_sum', 'snow_depth', 'season_id', 'is_operating_season', 'snow_depth_lag1', 'snow_depth_lag7', 'temperature_avg', 'temperature_avg_squared', 'resort_cortina_d_ampezzo', 'resort_kitzbuhel', 'resort_kranjska_gora', 'resort_krvavec', 'resort_les_trois_vallees', 'resort_mariborsko_pohorje', 'resort_sestriere', 'resort_solden', 'resort_st_anton', 'resort_st_moritz', 'resort_val_d_isere_tignes', 'resort_val_gardena', 'resort_verbier']\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encode the 'resort' feature\n",
    "model_data = pd.get_dummies(model_data, columns=['resort'], drop_first=True)\n",
    "\n",
    "# Display the columns to confirm encoding\n",
    "print(f\"Columns after encoding: {model_data.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preparing Features and Target Variable\n",
    "\n",
    "Features defined include: \n",
    "- temperature_avg\n",
    "- temperature_avg_squared\n",
    "- precipitation_sum\n",
    "- snow_depth_lag1\n",
    "- snow_depth_lag7\n",
    "- encoded resort\n",
    "\n",
    "Defining the target variable - snow_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable\n",
    "y = model_data['snow_depth']\n",
    "\n",
    "# Exclude 'date' and 'snow_depth' from features\n",
    "feature_columns = [col for col in model_data.columns if col not in ['date', 'snow_depth']]\n",
    "\n",
    "# Create the features DataFrame\n",
    "X = model_data[feature_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Handling Multicollinearity\n",
    "\n",
    "Before splitting the data, it's important to check for multicollinearity among features.\n",
    "\n",
    "Calculating the Variance Inflation Factor (VIF) for each feature is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/SkiSnow/venv/lib/python3.12/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Inflation Factors:\n",
      "                      feature       VIF\n",
      "0             temperature_min       inf\n",
      "1             temperature_max       inf\n",
      "6             temperature_avg       inf\n",
      "20             resort_verbier  7.483137\n",
      "16            resort_st_anton  6.477424\n",
      "9            resort_kitzbuhel  5.467867\n",
      "13  resort_mariborsko_pohorje  3.483182\n",
      "4             snow_depth_lag1  3.445501\n",
      "5             snow_depth_lag7  3.087779\n",
      "10       resort_kranjska_gora  3.034219\n",
      "7     temperature_avg_squared  2.887472\n",
      "15              resort_solden  2.832157\n",
      "3         is_operating_season  2.698546\n",
      "11             resort_krvavec  2.362995\n",
      "14           resort_sestriere  2.220728\n",
      "17           resort_st_moritz  2.095204\n",
      "18  resort_val_d_isere_tignes  1.925096\n",
      "12   resort_les_trois_vallees  1.701801\n",
      "19         resort_val_gardena  1.479474\n",
      "8    resort_cortina_d_ampezzo  1.471285\n",
      "2           precipitation_sum  1.133011\n"
     ]
    }
   ],
   "source": [
    "# Calculate VIF for each feature\n",
    "X_vif = X.copy()\n",
    "X_vif['Intercept'] = 1  # Add intercept term if necessary\n",
    "\n",
    "# Identify columns with object data type\n",
    "object_columns = X_vif.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Exclude 'season_id' from X_vif\n",
    "X_vif = X_vif.drop(columns=['season_id'])\n",
    "\n",
    "bool_columns = X_vif.select_dtypes(include=['bool']).columns.tolist()\n",
    "\n",
    "# Convert bool columns to int\n",
    "X_vif[bool_columns] = X_vif[bool_columns].astype(int)\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X_vif.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "\n",
    "# Drop the intercept term from VIF data\n",
    "vif_data = vif_data[vif_data['feature'] != 'Intercept']\n",
    "\n",
    "print(\"Variance Inflation Factors:\")\n",
    "print(vif_data.sort_values('VIF', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Splitting the Data\n",
    "\n",
    "Since the data is time-series data, it's important to split it in a way that respects the temporal order tand thus avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by date\n",
    "model_data = model_data.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Re-prepare X and y after sorting\n",
    "X = model_data[feature_columns]\n",
    "y = model_data['snow_depth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 (a) Split the data into training, validation, and test sets using time-based splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sizes for training, validation, and testing sets\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "test_size = len(X) - train_size - val_size\n",
    "\n",
    "# Split the data\n",
    "X_train = X.iloc[:train_size]\n",
    "y_train = y.iloc[:train_size]\n",
    "\n",
    "X_val = X.iloc[train_size:train_size + val_size]\n",
    "y_val = y.iloc[train_size:train_size + val_size]\n",
    "\n",
    "X_test = X.iloc[train_size + val_size:]\n",
    "y_test = y.iloc[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Proceeding with Modeling\n",
    "\n",
    "The data is now prepared.  We can proceed to build and evaluate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
