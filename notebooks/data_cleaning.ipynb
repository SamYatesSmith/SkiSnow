{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preparation\n",
    "\n",
    "In this notebook, we will perform data cleaning and preparation for all resorts across the Alps. This includes:\n",
    "\n",
    "- Loading the raw data\n",
    "- Handling missing values\n",
    "- Correcting data types\n",
    "- Normalizing resort names to handle special characters\n",
    "- Filtering data based on resort operating dates\n",
    "- Saving the cleaned data for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.cleaning import (\n",
    "    get_all_csv_files_with_metadata,\n",
    "    clean_and_filter_data,\n",
    "    save_cleaned_data\n",
    ")\n",
    "from src.features.feature_engineering import (\n",
    "    categorize_season,\n",
    "    add_operating_season_indicator\n",
    ")\n",
    "from src.features.anomaly_detection import (\n",
    "    detect_snow_depth_anomalies,\n",
    "    handle_snow_depth_anomalies\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define Resort Operating Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resort_seasons = {\n",
    "    'austrian_alps/st_anton': {'open': '12-01', 'close': '04-30'},\n",
    "    'austrian_alps/kitzbuhel': {'open': '10-15', 'close': '05-01'},\n",
    "    'austrian_alps/solden': {'open': '11-01', 'close': '05-01'},\n",
    "    'swiss_alps/st_moritz': {'open': '11-25', 'close': '05-01'},\n",
    "    'swiss_alps/verbier': {'open': '12-01', 'close': '04-30'},\n",
    "    'italian_alps/cortina_d_ampezzo': {'open': '11-25', 'close': '04-05'},\n",
    "    'italian_alps/val_gardena': {'open': '12-01', 'close': '04-15'},\n",
    "    'italian_alps/sestriere': {'open': '12-01', 'close': '04-15'},\n",
    "    'slovenian_alps/kranjska_gora': {'open': '12-15', 'close': '04-15'},\n",
    "    'slovenian_alps/mariborsko_pohorje': {'open': '12-01', 'close': '04-05'},\n",
    "    'slovenian_alps/krvavec': {'open': '12-01', 'close': '04-30'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root directory\n",
    "raw_data_root = '../data/raw/cds'\n",
    "processed_data_root = '../data/processed/cds'\n",
    "\n",
    "# Get list of all CSV files with dataset type\n",
    "csv_files = get_all_csv_files_with_metadata(raw_data_root)\n",
    "print(f\"Found {len(csv_files)} CSV files after excluding specified resorts.\")\n",
    "\n",
    "data_frames = {}\n",
    "for file_info in csv_files:\n",
    "    if file_info['type'] == 'new':  # Only process 'new' datasets\n",
    "        key, df = clean_and_filter_data(file_info)\n",
    "        if key and df is not None:\n",
    "            data_frames[key] = df\n",
    "            print(f\"Loaded and cleaned data for {key}: {df.shape[0]} rows.\")\n",
    "    else:\n",
    "        print(f\"Excluded 'old' dataset: {file_info['file_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering: Season Categorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in data_frames.items():\n",
    "    resort = key\n",
    "    if resort in resort_seasons:\n",
    "        season_info = resort_seasons[resort]\n",
    "        \n",
    "        # Categorize seasons\n",
    "        df = categorize_season(df, season_info, resort)\n",
    "        \n",
    "        # Add operating season indicator\n",
    "        df = add_operating_season_indicator(df)\n",
    "        \n",
    "        # Update the DataFrame in the dictionary\n",
    "        data_frames[key] = df\n",
    "        print(f\"Season categorized and operating season indicator added for {resort}.\")\n",
    "    else:\n",
    "        print(f\"No season information for {resort}. Data not categorized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Handle Missing Values and Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in data_frames.items():\n",
    "    # Impute missing 'snow_depth' during operating season\n",
    "    if 'snow_depth' in df.columns:\n",
    "        # Example imputation logic can be modularized further if needed\n",
    "        df['snow_depth'].fillna(method='ffill', inplace=True)\n",
    "        print(f\"{key}: Imputed missing 'snow_depth' values.\")\n",
    "    \n",
    "    # Detect and handle anomalies\n",
    "    df = detect_snow_depth_anomalies(df, threshold=20)\n",
    "    df = handle_snow_depth_anomalies(df)\n",
    "    \n",
    "    # Update the DataFrame in the dictionary\n",
    "    data_frames[key] = df\n",
    "    print(f\"Anomaly detection and handling completed for {key}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Rounding and Unit Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_round = {\n",
    "    'snow_depth': 1,\n",
    "    'precipitation_sum': 1,\n",
    "    'temperature_min': 1,\n",
    "    'temperature_max': 1,\n",
    "}\n",
    "\n",
    "for key, df in data_frames.items():\n",
    "    # Round numerical columns\n",
    "    for column, decimals in columns_to_round.items():\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].round(decimals)\n",
    "            print(f\"{key}: Rounded '{column}' to {decimals} decimal places.\")\n",
    "    \n",
    "    data_frames[key] = df\n",
    "    print(f\"Processed numerical columns for {key}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_cleaned_data(data_frames, processed_data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preparation\n",
    "\n",
    "In this notebook, we will perform data cleaning and preparation for all resorts across the Alps. This includes:\n",
    "\n",
    "- Loading the raw data\n",
    "- Handling missing values\n",
    "- Correcting data types\n",
    "- Normalizing resort names to handle special characters\n",
    "- Filtering data based on resort operating dates\n",
    "- Saving the cleaned data for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handling Special Characters in File and Resort Names\n",
    "\n",
    "To avoid issues with special characters (like accents and apostrophes) in file names and resort names, we'll define a normalization function. This function will:\n",
    "\n",
    "- Convert names to lowercase\n",
    "- Remove accents and diacritics\n",
    "- Replace non-alphanumeric characters with underscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_name(name):\n",
    "    \"\"\"\n",
    "    Normalize names by converting to lowercase, removing accents, and replacing non-alphanumeric characters with underscores.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    name = name.lower()\n",
    "    # Remove accents and diacritics\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    # Replace non-alphanumeric characters with underscores\n",
    "    name = re.sub(r'[^a-z0-9]+', '_', name)\n",
    "    # Remove leading/trailing underscores\n",
    "    name = name.strip('_')\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 (a) Utility function for standardising columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_columns(df):\n",
    "    \"\"\"\n",
    "    Standardize column names based on dataset type.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to standardize.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Standardized DataFrame.\n",
    "    \"\"\"\n",
    "    # Rename 'time' to 'date'\n",
    "    df = df.rename(columns={'time': 'date'})\n",
    "    \n",
    "    # Ensure 'precipitation_sum' and 'snow_depth' columns exist\n",
    "    if 'precipitation' in df.columns:\n",
    "        df = df.rename(columns={'precipitation': 'precipitation_sum'})\n",
    "    if 'snowfall' in df.columns:\n",
    "        df = df.rename(columns={'snowfall': 'snow_depth'})\n",
    "    \n",
    "    # If 'snow_depth' is missing, add it with NaN values\n",
    "    if 'snow_depth' not in df.columns:\n",
    "        df['snow_depth'] = np.nan\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 (b) Set data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_root = '../data/raw/cds'\n",
    "processed_data_root = '../data/processed/cds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Function to get list of all CSV files in the raw data directory\n",
    "\n",
    "We'll create a function to traverse the directory structure and collect all CSV files. While doing so, we'll normalize the country and resort names to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_csv_files_with_metadata(root_dir):\n",
    "\n",
    "    exclude_resorts = [\n",
    "        'french_alps/chamonix',\n",
    "        'french_alps/val_d_isere_tignes',\n",
    "        'french_alps/les_trois_vallees',\n",
    "        'swiss_alps/verbier',\n",
    "        'swiss_alps/zermatt'\n",
    "    ]\n",
    "\n",
    "    csv_files = []\n",
    "    for country in os.listdir(root_dir):\n",
    "        country_path = os.path.join(root_dir, country)\n",
    "        if os.path.isdir(country_path):\n",
    "            normalized_country = normalize_name(country)\n",
    "            for resort in os.listdir(country_path):\n",
    "                resort_path = os.path.join(country_path, resort)\n",
    "                if os.path.isdir(resort_path):\n",
    "                    normalized_resort = normalize_name(resort)\n",
    "                    key = f\"{normalized_country}/{normalized_resort}\"\n",
    "                    if key in exclude_resorts:\n",
    "                        print(f\"Excluding resort due to insufficient data: {key}\")\n",
    "                        continue  # Skip this resort\n",
    "                    for file in os.listdir(resort_path):\n",
    "                        if file.endswith('.csv'):\n",
    "                            file_path = os.path.join(resort_path, file)\n",
    "                            dataset_type = 'unknown'\n",
    "                            try:\n",
    "                                df_sample = pd.read_csv(file_path, nrows=1)\n",
    "                                columns = df_sample.columns.tolist()\n",
    "                                    \n",
    "                                # Check for 'new' dataset columns\n",
    "                                new_columns = {'temperature_min', 'temperature_max', 'precipitation_sum', 'snow_depth'}\n",
    "                                    \n",
    "                                if new_columns.issubset(columns):\n",
    "                                    dataset_type = 'new'\n",
    "                                else:\n",
    "                                    print(f\"Skipping 'old' dataset: {file_path}\")\n",
    "                                    continue  # Skip this file\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error reading {file_path}: {e}\")\n",
    "                                continue\n",
    "                                \n",
    "                            csv_files.append({\n",
    "                                'type': dataset_type,\n",
    "                                'country': normalized_country,\n",
    "                                'resort': normalized_resort,\n",
    "                                'file_path': file_path\n",
    "                            })\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning Steps\n",
    "\n",
    "We will perform the following data cleaning steps for each resort:\n",
    "\n",
    "1. Remove empty rows prior to `2021-03-23`\n",
    "2. Handle missing values\n",
    "3. Handle duplicates\n",
    "4. Correct data types\n",
    "5. Filter data based on each resort's opening and closing dates\n",
    "6. Save cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Function to Clean and Filter a Single CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_filter_data(file_info, optional_cutoff_date=None):\n",
    "    \"\"\"\n",
    "    Cleans and filters data based on dataset type.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_info (dict): Information about the file.\n",
    "    \n",
    "    Returns:\n",
    "    - key (str): Unique key for the resort.\n",
    "    - df (pd.DataFrame): Cleaned DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    country = file_info['country']\n",
    "    resort = file_info['resort']\n",
    "    file_path = file_info['file_path']\n",
    "\n",
    "    key = f\"{country}/{resort}\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = standardize_columns(df)\n",
    "        \n",
    "        # Convert 'date' column to datetime format\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        df = df.dropna(subset=['date'])  # Drop rows where date conversion failed\n",
    "        \n",
    "        # Apply optional cutoff date filter if specified\n",
    "        if optional_cutoff_date:\n",
    "            cutoff_date = pd.to_datetime(optional_cutoff_date)\n",
    "            df = df[df['date'] >= cutoff_date]\n",
    "\n",
    "            # Convert snow_depth units conditionally\n",
    "        if 'snow_depth' in df.columns:\n",
    "            if key == 'slovenian_alps/kranjska_gora':\n",
    "                df['snow_depth'] = df['snow_depth'] / 10  # Convert millimeters to centimeters\n",
    "                print(f\"{key}: Converted 'snow_depth' from millimeters to centimeters.\")\n",
    "            else:\n",
    "                print(f\"{key}: 'snow_depth' is assumed to be in centimeters. No conversion applied.\")\n",
    "        \n",
    "        df = df.reset_index(drop=True)\n",
    "        return key, df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Process All CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'old' dataset: ../data/raw/cds/austrian_alps/kitzbühel/kitzbühel.csv\n",
      "Skipping 'old' dataset: ../data/raw/cds/austrian_alps/st._anton/st._anton.csv\n",
      "Skipping 'old' dataset: ../data/raw/cds/austrian_alps/sölden/sölden.csv\n",
      "Excluding resort due to insufficient data: french_alps/chamonix\n",
      "Excluding resort due to insufficient data: french_alps/les_trois_vallees\n",
      "Excluding resort due to insufficient data: french_alps/val_d_isere_tignes\n",
      "Skipping 'old' dataset: ../data/raw/cds/italian_alps/cortina_d'ampezzo/cortina_d'ampezzo.csv\n",
      "Skipping 'old' dataset: ../data/raw/cds/italian_alps/sestriere/sestriere.csv\n",
      "Skipping 'old' dataset: ../data/raw/cds/italian_alps/val_gardena/val_gardena.csv\n",
      "Skipping 'old' dataset: ../data/raw/cds/slovenian_alps/kranjska_gora/kranjska_gora.csv\n",
      "Skipping 'old' dataset: ../data/raw/cds/slovenian_alps/krvavec/krvavec.csv\n",
      "Skipping 'old' dataset: ../data/raw/cds/slovenian_alps/mariborsko_pohorje/mariborsko_pohorje.csv\n",
      "Skipping 'old' dataset: ../data/raw/cds/swiss_alps/st._moritz/st._moritz.csv\n",
      "Excluding resort due to insufficient data: swiss_alps/verbier\n",
      "Excluding resort due to insufficient data: swiss_alps/zermatt\n",
      "Found 10 CSV files after excluding specified resorts.\n",
      "austrian_alps/kitzbuhel: 'snow_depth' is assumed to be in centimeters. No conversion applied.\n",
      "Loaded and cleaned data for austrian_alps/kitzbuhel: 11184 rows.\n",
      "austrian_alps/st_anton: 'snow_depth' is assumed to be in centimeters. No conversion applied.\n",
      "Loaded and cleaned data for austrian_alps/st_anton: 12418 rows.\n",
      "austrian_alps/solden: 'snow_depth' is assumed to be in centimeters. No conversion applied.\n",
      "Loaded and cleaned data for austrian_alps/solden: 12418 rows.\n",
      "italian_alps/cortina_d_ampezzo: 'snow_depth' is assumed to be in centimeters. No conversion applied.\n",
      "Loaded and cleaned data for italian_alps/cortina_d_ampezzo: 12015 rows.\n",
      "italian_alps/sestriere: 'snow_depth' is assumed to be in centimeters. No conversion applied.\n",
      "Loaded and cleaned data for italian_alps/sestriere: 12038 rows.\n",
      "italian_alps/val_gardena: 'snow_depth' is assumed to be in centimeters. No conversion applied.\n",
      "Loaded and cleaned data for italian_alps/val_gardena: 12015 rows.\n",
      "slovenian_alps/kranjska_gora: Converted 'snow_depth' from millimeters to centimeters.\n",
      "Loaded and cleaned data for slovenian_alps/kranjska_gora: 12418 rows.\n",
      "slovenian_alps/krvavec: 'snow_depth' is assumed to be in centimeters. No conversion applied.\n",
      "Loaded and cleaned data for slovenian_alps/krvavec: 12418 rows.\n",
      "slovenian_alps/mariborsko_pohorje: 'snow_depth' is assumed to be in centimeters. No conversion applied.\n",
      "Loaded and cleaned data for slovenian_alps/mariborsko_pohorje: 12418 rows.\n",
      "swiss_alps/st_moritz: 'snow_depth' is assumed to be in centimeters. No conversion applied.\n",
      "Loaded and cleaned data for swiss_alps/st_moritz: 12009 rows.\n"
     ]
    }
   ],
   "source": [
    "# Define the root directory\n",
    "raw_data_root = '../data/raw/cds'\n",
    "\n",
    "# Get list of all CSV files with dataset type\n",
    "csv_files = get_all_csv_files_with_metadata(raw_data_root)\n",
    "print(f\"Found {len(csv_files)} CSV files after excluding specified resorts.\")\n",
    "\n",
    "data_frames = {}\n",
    "for file_info in csv_files:\n",
    "    if file_info['type'] == 'new':  # Only process 'new' datasets\n",
    "        key, df = clean_and_filter_data(file_info)\n",
    "        if key and df is not None:\n",
    "            data_frames[key] = df\n",
    "            print(f\"Loaded and cleaned data for {key}: {df.shape[0]} rows.\")\n",
    "    else:\n",
    "        print(f\"Excluded 'old' dataset: {file_info['file_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Filter data based on each resort's opening and closing dates\n",
    "\n",
    "Each resort operates during specific dates in the year. We'll filter the data to include only the dates when each resort is open.\n",
    "\n",
    "Here are the approximate opening and closing dates for each resort:\n",
    "\n",
    "- **Austrian Alps:**\n",
    "  - **St. Anton:** Opens early December (`12-01`), closes late April (`04-30`)\n",
    "  - **Kitzbühel:** Opens mid-October (`10-15`), closes May (`05-01`)\n",
    "  - **Sölden:** Opens early November (`11-01`), closes early May (`05-01`)\n",
    "  \n",
    "- **Swiss Alps:**\n",
    "  - **Zermatt:** Opens mid-November (`11-15`), closes late April (`04-30`)\n",
    "  - **St. Moritz:** Opens late November (`11-25`), closes early May (`05-01`)\n",
    "  - **Verbier:** Opens early December (`12-01`), closes late April (`04-30`)\n",
    "  \n",
    "- **Italian Alps:**\n",
    "  - **Cortina d'Ampezzo:** Opens late November (`11-25`), closes early April (`04-05`)\n",
    "  - **Val Gardena:** Opens early December (`12-01`), closes mid-April (`04-15`)\n",
    "  - **Sestriere:** Opens early December (`12-01`), closes mid-April (`04-15`)\n",
    "  \n",
    "- **Slovenian Alps:**\n",
    "  - **Kranjska Gora:** Opens mid-December (`12-15`), closes mid-April (`04-15`)\n",
    "  - **Mariborsko Pohorje:** Opens December (`12-01`), closes early April (`04-05`)\n",
    "  - **Krvavec:** Opens December (`12-01`), closes April (`04-30`)\n",
    "\n",
    "  We'll define the `resort_seasons` dictionary with normalized keys to match the keys in `data_frames`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "resort_seasons = {\n",
    "    'austrian_alps/st_anton': {'open': '12-01', 'close': '04-30'},\n",
    "    'austrian_alps/kitzbuhel': {'open': '10-15', 'close': '05-01'},\n",
    "    'austrian_alps/solden': {'open': '11-01', 'close': '05-01'},\n",
    "    'swiss_alps/st_moritz': {'open': '11-25', 'close': '05-01'},\n",
    "    'swiss_alps/verbier': {'open': '12-01', 'close': '04-30'},\n",
    "    'italian_alps/cortina_d_ampezzo': {'open': '11-25', 'close': '04-05'},\n",
    "    'italian_alps/val_gardena': {'open': '12-01', 'close': '04-15'},\n",
    "    'italian_alps/sestriere': {'open': '12-01', 'close': '04-15'},\n",
    "    'slovenian_alps/kranjska_gora': {'open': '12-15', 'close': '04-15'},\n",
    "    'slovenian_alps/mariborsko_pohorje': {'open': '12-01', 'close': '04-05'},\n",
    "    'slovenian_alps/krvavec': {'open': '12-01', 'close': '04-30'},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Handles seasons that span across years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_dates(year, open_mm_dd, close_mm_dd):\n",
    "    \"\"\"\n",
    "    Given a year and open/close month-day strings, return datetime objects for open and close dates.\n",
    "    Handles seasons that span across years.\n",
    "    \"\"\"\n",
    "    open_month, open_day = map(int, open_mm_dd.split('-'))\n",
    "    close_month, close_day = map(int, close_mm_dd.split('-'))\n",
    "    \n",
    "    open_date = pd.Timestamp(year=year, month=open_month, day=open_day)\n",
    "    close_date = pd.Timestamp(year=year, month=close_month, day=close_day)\n",
    "    \n",
    "    # If close_date is earlier than open_date, it spans to the next year\n",
    "    if close_date < open_date:\n",
    "        close_date += pd.DateOffset(years=1)\n",
    "    \n",
    "    return open_date, close_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Assign rows to dataframe to a season based on the operating dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_season(df, season_info, resort_key):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing 'date' column.\n",
    "    - season_info (dict): Dictionary with 'open' and 'close' dates in 'MM-DD' format.\n",
    "    - resort_key (str): Key to identify the resort (e.g., 'french_alps/chamonix').\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with an added 'season_id' column.\n",
    "    \"\"\"\n",
    "    if not season_info:\n",
    "        # No season information provided\n",
    "        df['season_id'] = None\n",
    "        return df\n",
    "    \n",
    "    open_mm_dd = season_info['open']\n",
    "    close_mm_dd = season_info['close']\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['season_id'] = None  # Initialize season identifier\n",
    "    \n",
    "    years = df['date'].dt.year.unique()\n",
    "    \n",
    "    for year in years:\n",
    "        open_date, close_date = get_season_dates(year, open_mm_dd, close_mm_dd)\n",
    "        \n",
    "        # Filter rows within the current season\n",
    "        season_mask = (df['date'] >= open_date) & (df['date'] <= close_date)\n",
    "        season_label = f\"{year}-{close_date.year}\"\n",
    "        \n",
    "        df.loc[season_mask, 'season_id'] = season_label\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Apply Season Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season categorized for austrian_alps/kitzbuhel.\n",
      "Season categorized for austrian_alps/st_anton.\n",
      "Season categorized for austrian_alps/solden.\n",
      "Season categorized for italian_alps/cortina_d_ampezzo.\n",
      "Season categorized for italian_alps/sestriere.\n",
      "Season categorized for italian_alps/val_gardena.\n",
      "Season categorized for slovenian_alps/kranjska_gora.\n",
      "Season categorized for slovenian_alps/krvavec.\n",
      "Season categorized for slovenian_alps/mariborsko_pohorje.\n",
      "Season categorized for swiss_alps/st_moritz.\n"
     ]
    }
   ],
   "source": [
    "for key, df in data_frames.items():\n",
    "    resort = key\n",
    "    if resort in resort_seasons:\n",
    "        season_info = resort_seasons[resort]\n",
    "        \n",
    "        # Categorize seasons\n",
    "        df = categorize_season(df, season_info, resort)\n",
    "        \n",
    "        # Update the DataFrame in the dictionary\n",
    "        data_frames[key] = df\n",
    "        print(f\"Season categorized for {resort}.\")\n",
    "    else:\n",
    "        print(f\"No season information for {resort}. Data not categorized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Add Operating Season Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating season indicator added for austrian_alps/kitzbuhel.\n",
      "Operating season indicator added for austrian_alps/st_anton.\n",
      "Operating season indicator added for austrian_alps/solden.\n",
      "Operating season indicator added for italian_alps/cortina_d_ampezzo.\n",
      "Operating season indicator added for italian_alps/sestriere.\n",
      "Operating season indicator added for italian_alps/val_gardena.\n",
      "Operating season indicator added for slovenian_alps/kranjska_gora.\n",
      "Operating season indicator added for slovenian_alps/krvavec.\n",
      "Operating season indicator added for slovenian_alps/mariborsko_pohorje.\n",
      "Operating season indicator added for swiss_alps/st_moritz.\n"
     ]
    }
   ],
   "source": [
    "def add_operating_season_indicator(df):\n",
    "    \"\"\"\n",
    "    Adds a boolean column 'is_operating_season' indicating if the row is within an operating season.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['is_operating_season'] = df['season_id'].notnull()\n",
    "    return df\n",
    "\n",
    "for key, df in data_frames.items():\n",
    "    df = add_operating_season_indicator(df)\n",
    "    data_frames[key] = df\n",
    "    print(f\"Operating season indicator added for {key}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9  Impute Missing Values Appropriately\n",
    "\n",
    "Implement imputation methods based on dataset type.\n",
    "The code does the following: \n",
    "- Only impute snow depth increases when precipitation occurs during the operating season and conditions logically support snow accumulation.\n",
    "- Use patterns in the original dataset to guide imputation, focusing on years where similar weather conditions occurred.\n",
    "- Avoids imputation during off-season unless there is a strong indication of unusual snowfall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austrian_alps/kitzbuhel: Imputed 'snow_depth' based on historical average increase.\n",
      "austrian_alps/kitzbuhel: Applied temperature-based melting on days with no precipitation and above-freezing temperatures.\n",
      "austrian_alps/st_anton: Imputed 'snow_depth' based on historical average increase.\n",
      "austrian_alps/st_anton: Applied temperature-based melting on days with no precipitation and above-freezing temperatures.\n",
      "austrian_alps/solden: Imputed 'snow_depth' based on historical average increase.\n",
      "austrian_alps/solden: Applied temperature-based melting on days with no precipitation and above-freezing temperatures.\n",
      "italian_alps/cortina_d_ampezzo: Imputed 'snow_depth' based on historical average increase.\n",
      "italian_alps/cortina_d_ampezzo: Applied temperature-based melting on days with no precipitation and above-freezing temperatures.\n",
      "italian_alps/sestriere: Imputed 'snow_depth' based on historical average increase.\n",
      "italian_alps/sestriere: Applied temperature-based melting on days with no precipitation and above-freezing temperatures.\n",
      "italian_alps/val_gardena: Imputed 'snow_depth' based on historical average increase.\n",
      "italian_alps/val_gardena: Applied temperature-based melting on days with no precipitation and above-freezing temperatures.\n",
      "slovenian_alps/kranjska_gora: Imputed 'snow_depth' based on historical average increase.\n",
      "slovenian_alps/kranjska_gora: Applied temperature-based melting on days with no precipitation and above-freezing temperatures.\n",
      "slovenian_alps/krvavec: Imputed 'snow_depth' based on historical average increase.\n",
      "slovenian_alps/krvavec: Applied temperature-based melting on days with no precipitation and above-freezing temperatures.\n",
      "slovenian_alps/mariborsko_pohorje: Imputed 'snow_depth' based on historical average increase.\n",
      "slovenian_alps/mariborsko_pohorje: Applied temperature-based melting on days with no precipitation and above-freezing temperatures.\n",
      "swiss_alps/st_moritz: Imputed 'snow_depth' based on historical average increase.\n",
      "swiss_alps/st_moritz: Applied temperature-based melting on days with no precipitation and above-freezing temperatures.\n"
     ]
    }
   ],
   "source": [
    "for key, df in data_frames.items():\n",
    "    # Ensure the 'is_operating_season' column exists\n",
    "    if 'is_operating_season' not in df.columns:\n",
    "        raise ValueError(f\"'is_operating_season' column missing for {key}\")\n",
    "\n",
    "    # Filter for operating season\n",
    "    season_df = df[df['is_operating_season']]\n",
    "    \n",
    "    if 'snow_depth' in season_df.columns:\n",
    "        # Identify missing snow_depth values within the operating season\n",
    "        missing_mask = season_df['snow_depth'].isnull()\n",
    "        \n",
    "        # Conditions for snow depth increase imputation during freezing conditions\n",
    "        precip_freeze = (season_df['precipitation_sum'] > 0) & (season_df['temperature_min'] <= 0) & missing_mask\n",
    "        \n",
    "        if precip_freeze.any():\n",
    "            # Loop through data_frames to collect all historical data\n",
    "            historical_data = pd.concat([\n",
    "                other_df[(other_df['is_operating_season']) & \n",
    "                         (other_df['temperature_min'] <= 0) & \n",
    "                         (other_df['precipitation_sum'] > 0)]\n",
    "                for other_df in data_frames.values()\n",
    "            ], ignore_index=True)\n",
    "\n",
    "            if not historical_data.empty:\n",
    "                typical_increase = historical_data['snow_depth'].mean()\n",
    "                \n",
    "                if not np.isnan(typical_increase):\n",
    "                    season_df.loc[precip_freeze, 'snow_depth'] = typical_increase\n",
    "                    print(f\"{key}: Imputed 'snow_depth' based on historical average increase.\")\n",
    "                else:\n",
    "                    print(f\"{key}: Insufficient historical data for imputation.\")\n",
    "            else:\n",
    "                print(f\"{key}: No historical reference found for imputation under freezing conditions.\")\n",
    "        \n",
    "       # Conditions for days without precipitation (temperature-based melting)\n",
    "        no_precip = (season_df['precipitation_sum'] == 0) & missing_mask\n",
    "        \n",
    "        temp_melt_rates = pd.Series(0.0, index=season_df.index)  # Explicitly set to float\n",
    "\n",
    "        # Define temperature-adjusted melt rates based on specified ranges\n",
    "        temp_melt_rates[(season_df['temperature_max'] > 0) & (season_df['temperature_max'] <= 2)] = 0.005  # Minimal Melt\n",
    "        temp_melt_rates[(season_df['temperature_max'] > 2) & (season_df['temperature_max'] <= 5)] = 0.01  # Moderate Melt\n",
    "        temp_melt_rates[(season_df['temperature_max'] > 5) & (season_df['temperature_max'] <= 8)] = 0.015  # High Melt\n",
    "        temp_melt_rates[season_df['temperature_max'] > 8] = 0.02  # Maximum Melt Rate\n",
    "\n",
    "        if no_precip.any():\n",
    "            previous_snow_depth = season_df['snow_depth'].shift(1)\n",
    "            # Apply temperature-adjusted melt rates only on selected days\n",
    "            melt_mask = temp_melt_rates > 0\n",
    "            season_df.loc[no_precip & melt_mask, 'snow_depth'] = (\n",
    "                previous_snow_depth[no_precip & melt_mask].fillna(0) * \n",
    "                (1 - temp_melt_rates[no_precip & melt_mask])\n",
    "            )\n",
    "            print(f\"{key}: Applied temperature-based melting on days with no precipitation and above-freezing temperatures.\")\n",
    "\n",
    "        # Update the main DataFrame with only converted off-season data\n",
    "        df.update(season_df)\n",
    "\n",
    "    # Update the DataFrame in data_frames with imputed values for operating season and converted values for off-season\n",
    "    data_frames[key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 Incorporating anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_snow_depth_anomalies(df, threshold=20):\n",
    "    \"\"\"\n",
    "    Identifies snow_depth values that exceed a specified threshold during the off-season.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing 'snow_depth' and 'is_operating_season'.\n",
    "    - threshold (float): The maximum plausible snow_depth value during the off-season.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with anomalies flagged in 'snow_depth_anomaly' column.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure 'is_operating_season' column exists\n",
    "    if 'is_operating_season' not in df.columns:\n",
    "        raise ValueError(\"'is_operating_season' column is missing in the DataFrame.\")\n",
    "    \n",
    "    # Identify anomalies: snow_depth > threshold during off-season\n",
    "    off_season_mask = df['is_operating_season'] == False\n",
    "    anomaly_mask = (df['snow_depth'] > threshold) & off_season_mask\n",
    "    \n",
    "    df['snow_depth_anomaly'] = anomaly_mask\n",
    "    \n",
    "    return df\n",
    "\n",
    "for key, df in data_frames.items():\n",
    "    # Detect anomalies in 'snow_depth'\n",
    "    df = detect_snow_depth_anomalies(df, threshold=20)\n",
    "\n",
    "    # Remove or correct anomalies\n",
    "    df.loc[df['snow_depth_anomaly'], 'snow_depth'] = np.nan  # Set to NaN\n",
    "\n",
    "    # Drop the 'snow_depth_anomaly' column if not needed\n",
    "    df = df.drop(columns=['snow_depth_anomaly'])\n",
    "\n",
    "    # Update the DataFrame in the dictionary\n",
    "    data_frames[key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 Handle rounding and unit conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austrian_alps/kitzbuhel: Rounded 'snow_depth' to 1 decimal places.\n",
      "austrian_alps/kitzbuhel: Rounded 'precipitation_sum' to 1 decimal places.\n",
      "austrian_alps/kitzbuhel: Rounded 'temperature_min' to 1 decimal places.\n",
      "austrian_alps/kitzbuhel: Rounded 'temperature_max' to 1 decimal places.\n",
      "Processed numerical columns for austrian_alps/kitzbuhel.\n",
      "austrian_alps/st_anton: Rounded 'snow_depth' to 1 decimal places.\n",
      "austrian_alps/st_anton: Rounded 'precipitation_sum' to 1 decimal places.\n",
      "austrian_alps/st_anton: Rounded 'temperature_min' to 1 decimal places.\n",
      "austrian_alps/st_anton: Rounded 'temperature_max' to 1 decimal places.\n",
      "Processed numerical columns for austrian_alps/st_anton.\n",
      "austrian_alps/solden: Rounded 'snow_depth' to 1 decimal places.\n",
      "austrian_alps/solden: Rounded 'precipitation_sum' to 1 decimal places.\n",
      "austrian_alps/solden: Rounded 'temperature_min' to 1 decimal places.\n",
      "austrian_alps/solden: Rounded 'temperature_max' to 1 decimal places.\n",
      "Processed numerical columns for austrian_alps/solden.\n",
      "italian_alps/cortina_d_ampezzo: Rounded 'snow_depth' to 1 decimal places.\n",
      "italian_alps/cortina_d_ampezzo: Rounded 'precipitation_sum' to 1 decimal places.\n",
      "italian_alps/cortina_d_ampezzo: Rounded 'temperature_min' to 1 decimal places.\n",
      "italian_alps/cortina_d_ampezzo: Rounded 'temperature_max' to 1 decimal places.\n",
      "Processed numerical columns for italian_alps/cortina_d_ampezzo.\n",
      "italian_alps/sestriere: Rounded 'snow_depth' to 1 decimal places.\n",
      "italian_alps/sestriere: Rounded 'precipitation_sum' to 1 decimal places.\n",
      "italian_alps/sestriere: Rounded 'temperature_min' to 1 decimal places.\n",
      "italian_alps/sestriere: Rounded 'temperature_max' to 1 decimal places.\n",
      "Processed numerical columns for italian_alps/sestriere.\n",
      "italian_alps/val_gardena: Rounded 'snow_depth' to 1 decimal places.\n",
      "italian_alps/val_gardena: Rounded 'precipitation_sum' to 1 decimal places.\n",
      "italian_alps/val_gardena: Rounded 'temperature_min' to 1 decimal places.\n",
      "italian_alps/val_gardena: Rounded 'temperature_max' to 1 decimal places.\n",
      "Processed numerical columns for italian_alps/val_gardena.\n",
      "slovenian_alps/kranjska_gora: Rounded 'snow_depth' to 1 decimal places.\n",
      "slovenian_alps/kranjska_gora: Rounded 'precipitation_sum' to 1 decimal places.\n",
      "slovenian_alps/kranjska_gora: Rounded 'temperature_min' to 1 decimal places.\n",
      "slovenian_alps/kranjska_gora: Rounded 'temperature_max' to 1 decimal places.\n",
      "Processed numerical columns for slovenian_alps/kranjska_gora.\n",
      "slovenian_alps/krvavec: Rounded 'snow_depth' to 1 decimal places.\n",
      "slovenian_alps/krvavec: Rounded 'precipitation_sum' to 1 decimal places.\n",
      "slovenian_alps/krvavec: Rounded 'temperature_min' to 1 decimal places.\n",
      "slovenian_alps/krvavec: Rounded 'temperature_max' to 1 decimal places.\n",
      "Processed numerical columns for slovenian_alps/krvavec.\n",
      "slovenian_alps/mariborsko_pohorje: Rounded 'snow_depth' to 1 decimal places.\n",
      "slovenian_alps/mariborsko_pohorje: Rounded 'precipitation_sum' to 1 decimal places.\n",
      "slovenian_alps/mariborsko_pohorje: Rounded 'temperature_min' to 1 decimal places.\n",
      "slovenian_alps/mariborsko_pohorje: Rounded 'temperature_max' to 1 decimal places.\n",
      "Processed numerical columns for slovenian_alps/mariborsko_pohorje.\n",
      "swiss_alps/st_moritz: Rounded 'snow_depth' to 1 decimal places.\n",
      "swiss_alps/st_moritz: Rounded 'precipitation_sum' to 1 decimal places.\n",
      "swiss_alps/st_moritz: Rounded 'temperature_min' to 1 decimal places.\n",
      "swiss_alps/st_moritz: Rounded 'temperature_max' to 1 decimal places.\n",
      "Processed numerical columns for swiss_alps/st_moritz.\n"
     ]
    }
   ],
   "source": [
    "columns_to_round = {\n",
    "    'snow_depth': 1,\n",
    "    'precipitation_sum': 1,\n",
    "    'temperature_min': 1,\n",
    "    'temperature_max': 1,\n",
    "}\n",
    "\n",
    "for key, df in data_frames.items():\n",
    "    # Round numerical columns\n",
    "    for column, decimals in columns_to_round.items():\n",
    "        if column in df.columns:\n",
    "            df[column] = df[column].round(decimals)\n",
    "            print(f\"{key}: Rounded '{column}' to {decimals} decimal places.\")\n",
    "\n",
    "    data_frames[key] = df\n",
    "    print(f\"Processed numerical columns for {key}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Cleaned Data\n",
    "\n",
    "We'll save the cleaned and filtered DataFrames to the `data/processed` directory, maintaining the normalized folder structure.\n",
    "Furthermore, we need to save to unique filenames, given that we have previously cleaned data within the respective folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned data to ../data/processed/cds/austrian_alps/kitzbuhel/kitzbuhel_cleaned_2024-10-15_12-03-53.csv.\n",
      "Saved cleaned data to ../data/processed/cds/austrian_alps/st_anton/st_anton_cleaned_2024-10-15_12-03-53.csv.\n",
      "Saved cleaned data to ../data/processed/cds/austrian_alps/solden/solden_cleaned_2024-10-15_12-03-53.csv.\n",
      "Saved cleaned data to ../data/processed/cds/italian_alps/cortina_d_ampezzo/cortina_d_ampezzo_cleaned_2024-10-15_12-03-53.csv.\n",
      "Saved cleaned data to ../data/processed/cds/italian_alps/sestriere/sestriere_cleaned_2024-10-15_12-03-53.csv.\n",
      "Saved cleaned data to ../data/processed/cds/italian_alps/val_gardena/val_gardena_cleaned_2024-10-15_12-03-53.csv.\n",
      "Saved cleaned data to ../data/processed/cds/slovenian_alps/kranjska_gora/kranjska_gora_cleaned_2024-10-15_12-03-53.csv.\n",
      "Saved cleaned data to ../data/processed/cds/slovenian_alps/krvavec/krvavec_cleaned_2024-10-15_12-03-53.csv.\n",
      "Saved cleaned data to ../data/processed/cds/slovenian_alps/mariborsko_pohorje/mariborsko_pohorje_cleaned_2024-10-15_12-03-53.csv.\n",
      "Saved cleaned data to ../data/processed/cds/swiss_alps/st_moritz/st_moritz_cleaned_2024-10-15_12-03-53.csv.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime  # Import datetime for timestamp generation\n",
    "\n",
    "for key, df in data_frames.items():\n",
    "    try:\n",
    "        # Split the key back into country and resort\n",
    "        country, resort = key.split('/')\n",
    "        \n",
    "        # Build the processed data path\n",
    "        processed_dir = os.path.join(processed_data_root, country, resort)\n",
    "        os.makedirs(processed_dir, exist_ok=True)\n",
    "        \n",
    "        # Generate current timestamp in 'YYYY-MM-DD_HH-MM-SS' format\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        \n",
    "        # Define the new file name with timestamp to prevent overwriting\n",
    "        processed_file_path = os.path.join(processed_dir, f\"{resort}_cleaned_{timestamp}.csv\")\n",
    "        \n",
    "        # Save the cleaned DataFrame to the new CSV file\n",
    "        df.to_csv(processed_file_path, index=False)\n",
    "        \n",
    "        # Informative message indicating successful save\n",
    "        print(f\"Saved cleaned data to {processed_file_path}.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle potential errors, such as key not having exactly two parts\n",
    "        print(f\"Error saving data for {key}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
