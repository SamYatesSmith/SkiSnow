{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preparation\n",
    "\n",
    "In this notebook, we will perform data cleaning and preparation for all resorts across the Alps. This includes:\n",
    "\n",
    "- Loading the raw data\n",
    "- Handling missing values\n",
    "- Correcting data types\n",
    "- Normalizing resort names to handle special characters\n",
    "- Filtering data based on resort operating dates\n",
    "- Saving the cleaned data for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handling Special Characters in File and Resort Names\n",
    "\n",
    "To avoid issues with special characters (like accents and apostrophes) in file names and resort names, we'll define a normalization function. This function will:\n",
    "\n",
    "- Convert names to lowercase\n",
    "- Remove accents and diacritics\n",
    "- Replace non-alphanumeric characters with underscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_name(name):\n",
    "    # Convert to lowercase\n",
    "    name = name.lower()\n",
    "    # Remove accents and diacritics\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    # Replace non-alphanumeric characters with underscores\n",
    "    name = re.sub(r'[^a-z0-9]+', '_', name)\n",
    "    # Remove leading/trailing underscores\n",
    "    name = name.strip('_')\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(a) Set Paths for Raw and Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_root = '../data/raw/cds'\n",
    "processed_data_root = '../data/processed/cds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Function to get list of all CSV files in the raw data directory\n",
    "\n",
    "We'll create a function to traverse the directory structure and collect all CSV files. While doing so, we'll normalize the country and resort names to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 CSV files.\n"
     ]
    }
   ],
   "source": [
    "def get_all_csv_files(root_dir):\n",
    "    csv_files = []\n",
    "    for country in os.listdir(root_dir):\n",
    "        country_path = os.path.join(root_dir, country)\n",
    "        if os.path.isdir(country_path):\n",
    "            normalized_country = normalize_name(country)\n",
    "            for resort in os.listdir(country_path):\n",
    "                resort_path = os.path.join(country_path, resort)\n",
    "                if os.path.isdir(resort_path):\n",
    "                    normalized_resort = normalize_name(resort)\n",
    "                    for file in os.listdir(resort_path):\n",
    "                        if file.endswith('.csv'):\n",
    "                            file_path = os.path.join(resort_path, file)\n",
    "                            csv_files.append({\n",
    "                                'country': normalized_country,\n",
    "                                'resort': normalized_resort,\n",
    "                                'file_path': file_path\n",
    "                            })\n",
    "    return csv_files\n",
    "\n",
    "# Get list of all CSV files\n",
    "csv_files = get_all_csv_files(raw_data_root)\n",
    "print(f\"Found {len(csv_files)} CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning Steps\n",
    "\n",
    "We will perform the following data cleaning steps for each resort:\n",
    "\n",
    "1. Remove empty rows prior to `2021-03-23`\n",
    "2. Handle missing values\n",
    "3. Handle duplicates\n",
    "4. Correct data types\n",
    "5. Filter data based on each resort's opening and closing dates\n",
    "6. Save cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Function to Clean and Filter a Single CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_filter_data(file_info):\n",
    "    country = file_info['country']\n",
    "    resort = file_info['resort']\n",
    "    file_path = file_info['file_path']\n",
    "    \n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert 'date' column to datetime format and remove timezone info\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.tz_localize(None)\n",
    "    \n",
    "    # Drop rows with missing dates\n",
    "    df = df.dropna(subset=['date'])\n",
    "    \n",
    "    # Remove rows prior to 2021-03-23\n",
    "    df = df[df['date'] >= '2021-03-23']\n",
    "    \n",
    "    # Handle missing values in other columns\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Reset index after dropping rows\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Normalize the resort key\n",
    "    key = f\"{country}/{resort}\"\n",
    "    \n",
    "    return key, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Process All CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and cleaned data for austrian_alps/kitzbuhel: 1166 rows.\n",
      "Loaded and cleaned data for austrian_alps/st_anton: 1166 rows.\n",
      "Loaded and cleaned data for austrian_alps/solden: 1166 rows.\n",
      "Loaded and cleaned data for french_alps/chamonix: 1166 rows.\n",
      "Loaded and cleaned data for french_alps/les_trois_vallees: 1166 rows.\n",
      "Loaded and cleaned data for french_alps/val_d_isere_tignes: 1166 rows.\n",
      "Loaded and cleaned data for italian_alps/cortina_d_ampezzo: 1166 rows.\n",
      "Loaded and cleaned data for italian_alps/sestriere: 1166 rows.\n",
      "Loaded and cleaned data for italian_alps/val_gardena: 1166 rows.\n",
      "Loaded and cleaned data for slovenian_alps/kranjska_gora: 1166 rows.\n",
      "Loaded and cleaned data for slovenian_alps/krvavec: 1166 rows.\n",
      "Loaded and cleaned data for slovenian_alps/mariborsko_pohorje: 1166 rows.\n",
      "Loaded and cleaned data for swiss_alps/st_moritz: 1166 rows.\n",
      "Loaded and cleaned data for swiss_alps/verbier: 1166 rows.\n",
      "Loaded and cleaned data for swiss_alps/zermatt: 1166 rows.\n"
     ]
    }
   ],
   "source": [
    "data_frames = {}\n",
    "\n",
    "for file_info in csv_files:\n",
    "    key, df = clean_and_filter_data(file_info)\n",
    "    data_frames[key] = df\n",
    "    print(f\"Loaded and cleaned data for {key}: {df.shape[0]} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Filter data based on each resort's opening and closing dates\n",
    "\n",
    "Each resort operates during specific dates in the year. We'll filter the data to include only the dates when each resort is open.\n",
    "\n",
    "Here are the approximate opening and closing dates for each resort:\n",
    "\n",
    "- **French Alps:**\n",
    "  - **Chamonix:** Opens mid-December (`12-15`), closes mid-May (`05-15`)\n",
    "  - **Val d'Isère & Tignes:** Opens November 30 (`11-30`), closes May 5 (`05-05`)\n",
    "  - **Les Trois Vallées:** Opens December 7 (`12-07`), closes mid-April (`04-15`)\n",
    "  \n",
    "- **Austrian Alps:**\n",
    "  - **St. Anton:** Opens early December (`12-01`), closes late April (`04-30`)\n",
    "  - **Kitzbühel:** Opens mid-October (`10-15`), closes May (`05-01`)\n",
    "  - **Sölden:** Opens early November (`11-01`), closes early May (`05-01`)\n",
    "  \n",
    "- **Swiss Alps:**\n",
    "  - **Zermatt:** Opens mid-November (`11-15`), closes late April (`04-30`)\n",
    "  - **St. Moritz:** Opens late November (`11-25`), closes early May (`05-01`)\n",
    "  - **Verbier:** Opens early December (`12-01`), closes late April (`04-30`)\n",
    "  \n",
    "- **Italian Alps:**\n",
    "  - **Cortina d'Ampezzo:** Opens late November (`11-25`), closes early April (`04-05`)\n",
    "  - **Val Gardena:** Opens early December (`12-01`), closes mid-April (`04-15`)\n",
    "  - **Sestriere:** Opens early December (`12-01`), closes mid-April (`04-15`)\n",
    "  \n",
    "- **Slovenian Alps:**\n",
    "  - **Kranjska Gora:** Opens mid-December (`12-15`), closes mid-April (`04-15`)\n",
    "  - **Mariborsko Pohorje:** Opens December (`12-01`), closes early April (`04-05`)\n",
    "  - **Krvavec:** Opens December (`12-01`), closes April (`04-30`)\n",
    "\n",
    "  We'll define the `resort_seasons` dictionary with normalized keys to match the keys in `data_frames`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "resort_seasons = {\n",
    "    'french_alps/chamonix': {'open': '12-15', 'close': '05-15'},\n",
    "    'french_alps/val_d_isere_tignes': {'open': '11-30', 'close': '05-05'},\n",
    "    'french_alps/les_trois_vallees': {'open': '12-07', 'close': '04-15'},\n",
    "    'austrian_alps/st_anton': {'open': '12-01', 'close': '04-30'},\n",
    "    'austrian_alps/kitzbuhel': {'open': '10-15', 'close': '05-01'},\n",
    "    'austrian_alps/solden': {'open': '11-01', 'close': '05-01'},\n",
    "    'swiss_alps/zermatt': {'open': '11-15', 'close': '04-30'},\n",
    "    'swiss_alps/st_moritz': {'open': '11-25', 'close': '05-01'},\n",
    "    'swiss_alps/verbier': {'open': '12-01', 'close': '04-30'},\n",
    "    'italian_alps/cortina_d_ampezzo': {'open': '11-25', 'close': '04-05'},\n",
    "    'italian_alps/val_gardena': {'open': '12-01', 'close': '04-15'},\n",
    "    'italian_alps/sestriere': {'open': '12-01', 'close': '04-15'},\n",
    "    'slovenian_alps/kranjska_gora': {'open': '12-15', 'close': '04-15'},\n",
    "    'slovenian_alps/mariborsko_pohorje': {'open': '12-01', 'close': '04-05'},\n",
    "    'slovenian_alps/krvavec': {'open': '12-01', 'close': '04-30'},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4/5 Filter Data Based on Resort Operating Dates\n",
    "\n",
    "We'll filter each resort's data to include only dates within its operating season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data for austrian_alps/kitzbuhel: 595 rows within operating dates.\n",
      "Filtered data for austrian_alps/st_anton: 451 rows within operating dates.\n",
      "Filtered data for austrian_alps/solden: 544 rows within operating dates.\n",
      "Filtered data for french_alps/chamonix: 454 rows within operating dates.\n",
      "Filtered data for french_alps/les_trois_vallees: 388 rows within operating dates.\n",
      "Filtered data for french_alps/val_d_isere_tignes: 469 rows within operating dates.\n",
      "Filtered data for italian_alps/cortina_d_ampezzo: 394 rows within operating dates.\n",
      "Filtered data for italian_alps/sestriere: 406 rows within operating dates.\n",
      "Filtered data for italian_alps/val_gardena: 406 rows within operating dates.\n",
      "Filtered data for slovenian_alps/kranjska_gora: 364 rows within operating dates.\n",
      "Filtered data for slovenian_alps/krvavec: 451 rows within operating dates.\n",
      "Filtered data for slovenian_alps/mariborsko_pohorje: 376 rows within operating dates.\n",
      "Filtered data for swiss_alps/st_moritz: 472 rows within operating dates.\n",
      "Filtered data for swiss_alps/verbier: 451 rows within operating dates.\n",
      "Filtered data for swiss_alps/zermatt: 499 rows within operating dates.\n"
     ]
    }
   ],
   "source": [
    "for key, df in data_frames.items():\n",
    "    resort = key\n",
    "    if resort in resort_seasons:\n",
    "        season = resort_seasons[resort]\n",
    "        open_month_day = season['open']\n",
    "        close_month_day = season['close']\n",
    "        \n",
    "        # Since the data spans multiple years, we need to filter for each year\n",
    "        df['year'] = df['date'].dt.year\n",
    "        filtered_dfs = []\n",
    "        \n",
    "        for year in df['year'].unique():\n",
    "            open_date_str = f\"{year}-{open_month_day}\"\n",
    "            close_date_str = f\"{year}-{close_month_day}\"\n",
    "            open_date = pd.to_datetime(open_date_str, errors='coerce').tz_localize(None)\n",
    "            close_date = pd.to_datetime(close_date_str, errors='coerce').tz_localize(None)\n",
    "            \n",
    "            # Handle cases where the season spans over the new year\n",
    "            if close_date < open_date:\n",
    "                # Season spans over to the next year\n",
    "                close_date += relativedelta(years=1)\n",
    "            \n",
    "            season_df = df[(df['date'] >= open_date) & (df['date'] <= close_date)]\n",
    "            filtered_dfs.append(season_df)\n",
    "        \n",
    "        # Combine all seasons\n",
    "        df_season = pd.concat(filtered_dfs)\n",
    "        \n",
    "        # Drop the 'year' column\n",
    "        df_season = df_season.drop(columns=['year'])\n",
    "        \n",
    "        # Update the DataFrame in the dictionary\n",
    "        data_frames[key] = df_season.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Filtered data for {resort}: {df_season.shape[0]} rows within operating dates.\")\n",
    "    else:\n",
    "        print(f\"No season information for {resort}. Data not filtered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Cleaned Data\n",
    "\n",
    "We'll save the cleaned and filtered DataFrames to the `data/processed` directory, maintaining the normalized folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned data to ../data/processed/cds/austrian_alps/kitzbuhel/kitzbuhel_cleaned.csv.\n",
      "Saved cleaned data to ../data/processed/cds/austrian_alps/st_anton/st_anton_cleaned.csv.\n",
      "Saved cleaned data to ../data/processed/cds/austrian_alps/solden/solden_cleaned.csv.\n",
      "Saved cleaned data to ../data/processed/cds/french_alps/chamonix/chamonix_cleaned.csv.\n",
      "Saved cleaned data to ../data/processed/cds/french_alps/les_trois_vallees/les_trois_vallees_cleaned.csv.\n",
      "Saved cleaned data to ../data/processed/cds/french_alps/val_d_isere_tignes/val_d_isere_tignes_cleaned.csv.\n",
      "Saved cleaned data to ../data/processed/cds/italian_alps/cortina_d_ampezzo/cortina_d_ampezzo_cleaned.csv.\n",
      "Saved cleaned data to ../data/processed/cds/italian_alps/sestriere/sestriere_cleaned.csv.\n",
      "Saved cleaned data to ../data/processed/cds/italian_alps/val_gardena/val_gardena_cleaned.csv.\n",
      "Saved cleaned data to ../data/processed/cds/slovenian_alps/kranjska_gora/kranjska_gora_cleaned.csv.\n",
      "Saved cleaned data to ../data/processed/cds/slovenian_alps/krvavec/krvavec_cleaned.csv.\n",
      "Saved cleaned data to ../data/processed/cds/slovenian_alps/mariborsko_pohorje/mariborsko_pohorje_cleaned.csv.\n",
      "Saved cleaned data to ../data/processed/cds/swiss_alps/st_moritz/st_moritz_cleaned.csv.\n",
      "Saved cleaned data to ../data/processed/cds/swiss_alps/verbier/verbier_cleaned.csv.\n",
      "Saved cleaned data to ../data/processed/cds/swiss_alps/zermatt/zermatt_cleaned.csv.\n"
     ]
    }
   ],
   "source": [
    "for key, df in data_frames.items():\n",
    "    # Split the key back into country and resort\n",
    "    country, resort = key.split('/')\n",
    "    # Build the processed data path\n",
    "    processed_dir = os.path.join(processed_data_root, country, resort)\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    # Save the cleaned DataFrame\n",
    "    processed_file_path = os.path.join(processed_dir, f\"{resort}_cleaned.csv\")\n",
    "    df.to_csv(processed_file_path, index=False)\n",
    "    print(f\"Saved cleaned data to {processed_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "- Loaded and cleaned data for all resorts.\n",
    "- Normalized resort names to handle special characters.\n",
    "- Filtered data based on each resort's operating dates.\n",
    "- Saved cleaned data to the `data/processed` directory.\n",
    "\n",
    "The cleaned datasets are now ready for feature engineering and further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
