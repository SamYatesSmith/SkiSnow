{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preparation\n",
    "\n",
    "In this notebook, we will perform data cleaning and preparation for all resorts across the Alps. This includes:\n",
    "\n",
    "- Loading the raw data\n",
    "- Handling missing values\n",
    "- Correcting data types\n",
    "- Normalizing resort names to handle special characters\n",
    "- Filtering data based on resort operating dates\n",
    "- Saving the cleaned data for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handling Special Characters in File and Resort Names\n",
    "\n",
    "To avoid issues with special characters (like accents and apostrophes) in file names and resort names, we'll define a normalization function. This function will:\n",
    "\n",
    "- Convert names to lowercase\n",
    "- Remove accents and diacritics\n",
    "- Replace non-alphanumeric characters with underscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_name(name):\n",
    "    \"\"\"\n",
    "    Normalize names by converting to lowercase, removing accents, and replacing non-alphanumeric characters with underscores.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    name = name.lower()\n",
    "    # Remove accents and diacritics\n",
    "    name = unicodedata.normalize('NFKD', name).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    # Replace non-alphanumeric characters with underscores\n",
    "    name = re.sub(r'[^a-z0-9]+', '_', name)\n",
    "    # Remove leading/trailing underscores\n",
    "    name = name.strip('_')\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 (a) Utility function for standardising columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_columns(df, dataset_type):\n",
    "    \"\"\"\n",
    "    Standardize column names based on dataset type.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to standardize.\n",
    "    - dataset_type (str): 'old' or 'new'.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Standardized DataFrame.\n",
    "    \"\"\"\n",
    "    if dataset_type == 'new':\n",
    "        # Rename 'time' to 'date'\n",
    "        df = df.rename(columns={'time': 'date'})\n",
    "        # No renaming needed for other columns\n",
    "    elif dataset_type == 'old':\n",
    "        # Rename columns to standard names\n",
    "        column_mappings = {\n",
    "            'temperature_2m_max': 'temperature_max',\n",
    "            'temperature_2m_min': 'temperature_min',\n",
    "            'rain_sum': 'precipitation_sum',\n",
    "        }\n",
    "        df = df.rename(columns=column_mappings)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 (b) Set data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_root = '../data/raw/cds'\n",
    "processed_data_root = '../data/processed/cds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Function to get list of all CSV files in the raw data directory\n",
    "\n",
    "We'll create a function to traverse the directory structure and collect all CSV files. While doing so, we'll normalize the country and resort names to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_csv_files_with_metadata(root_dir):\n",
    "    csv_files = []\n",
    "    for country in os.listdir(root_dir):\n",
    "        country_path = os.path.join(root_dir, country)\n",
    "        if os.path.isdir(country_path):\n",
    "            normalized_country = normalize_name(country)\n",
    "            for resort in os.listdir(country_path):\n",
    "                resort_path = os.path.join(country_path, resort)\n",
    "                if os.path.isdir(resort_path):\n",
    "                    normalized_resort = normalize_name(resort)\n",
    "                    for file in os.listdir(resort_path):\n",
    "                        if file.endswith('.csv'):\n",
    "                            file_path = os.path.join(resort_path, file)\n",
    "                            dataset_type = 'unknown'\n",
    "                            try:\n",
    "                                df_sample = pd.read_csv(file_path, nrows=1)\n",
    "                                if 'time' in df_sample.columns:\n",
    "                                    dataset_type = 'new'\n",
    "                                elif 'date' in df_sample.columns:\n",
    "                                    dataset_type = 'old'\n",
    "                                else:\n",
    "                                    print(f\"File {file_path} does not contain 'time' or 'date' column. Skipping.\")\n",
    "                                    continue  # Skip this file\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error reading {file_path}: {e}\")\n",
    "                                continue  # Skip this file\n",
    "                            \n",
    "                            csv_files.append({\n",
    "                                'type': dataset_type,  # 'old' or 'new'\n",
    "                                'country': normalized_country,\n",
    "                                'resort': normalized_resort,\n",
    "                                'file_path': file_path\n",
    "                            })\n",
    "    return csv_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning Steps\n",
    "\n",
    "We will perform the following data cleaning steps for each resort:\n",
    "\n",
    "1. Remove empty rows prior to `2021-03-23`\n",
    "2. Handle missing values\n",
    "3. Handle duplicates\n",
    "4. Correct data types\n",
    "5. Filter data based on each resort's opening and closing dates\n",
    "6. Save cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Function to Clean and Filter a Single CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_filter_data(file_info):\n",
    "    \"\"\"\n",
    "    Cleans and filters data based on dataset type.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_info (dict): Information about the file.\n",
    "    \n",
    "    Returns:\n",
    "    - key (str): Unique key for the resort.\n",
    "    - df (pd.DataFrame): Cleaned DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    country = file_info['country']\n",
    "    resort = file_info['resort']\n",
    "    dataset_type = file_info['type']  # 'old' or 'new'\n",
    "    file_path = file_info['file_path']\n",
    "    \n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Standardize column names based on dataset type\n",
    "        df = standardize_columns(df, dataset_type)\n",
    "        \n",
    "        # Convert 'date' column to datetime format and remove timezone info\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "            if dataset_type == 'old':\n",
    "                # Remove timezone information if present\n",
    "                df['date'] = df['date'].dt.tz_localize(None) if df['date'].dt.tz else df['date']\n",
    "        else:\n",
    "            print(f\"'date' column not found in {file_path}. Skipping date parsing.\")\n",
    "            df['date'] = pd.NaT  # Assign NaT if 'date' is missing\n",
    "    \n",
    "        df = df.dropna(subset=['date'])\n",
    "        if dataset_type == 'old':\n",
    "            # Remove rows prior to 2021-03-23\n",
    "            df = df[df['date'] >= '2021-03-23']\n",
    "    \n",
    "        df = df.reset_index(drop=True)\n",
    "        key = f\"{country}/{resort}\"\n",
    "    \n",
    "        return key, df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Process All CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 CSV files (Old: 24, New: 6).\n",
      "Loaded and cleaned data for austrian_alps/kitzbuhel: 11184 rows.\n",
      "Loaded and cleaned data for austrian_alps/kitzbuhel: 1166 rows.\n",
      "Loaded and cleaned data for austrian_alps/st_anton: 12418 rows.\n",
      "Loaded and cleaned data for austrian_alps/st_anton: 1166 rows.\n",
      "Loaded and cleaned data for austrian_alps/solden: 12418 rows.\n",
      "Loaded and cleaned data for austrian_alps/solden: 1166 rows.\n",
      "Loaded and cleaned data for french_alps/chamonix: 1166 rows.\n",
      "Loaded and cleaned data for french_alps/chamonix: 12418 rows.\n",
      "Loaded and cleaned data for french_alps/les_trois_vallees: 12418 rows.\n",
      "Loaded and cleaned data for french_alps/les_trois_vallees: 1166 rows.\n",
      "Loaded and cleaned data for french_alps/val_d_isere_tignes: 12418 rows.\n",
      "Loaded and cleaned data for french_alps/val_d_isere_tignes: 1166 rows.\n",
      "Loaded and cleaned data for italian_alps/cortina_d_ampezzo: 1166 rows.\n",
      "Loaded and cleaned data for italian_alps/cortina_d_ampezzo: 1014 rows.\n",
      "Loaded and cleaned data for italian_alps/sestriere: 1014 rows.\n",
      "Loaded and cleaned data for italian_alps/sestriere: 1166 rows.\n",
      "Loaded and cleaned data for italian_alps/val_gardena: 1014 rows.\n",
      "Loaded and cleaned data for italian_alps/val_gardena: 1166 rows.\n",
      "Loaded and cleaned data for slovenian_alps/kranjska_gora: 1166 rows.\n",
      "Loaded and cleaned data for slovenian_alps/kranjska_gora: 1014 rows.\n",
      "Loaded and cleaned data for slovenian_alps/krvavec: 1166 rows.\n",
      "Loaded and cleaned data for slovenian_alps/krvavec: 1014 rows.\n",
      "Loaded and cleaned data for slovenian_alps/mariborsko_pohorje: 1166 rows.\n",
      "Loaded and cleaned data for slovenian_alps/mariborsko_pohorje: 1014 rows.\n",
      "Loaded and cleaned data for swiss_alps/st_moritz: 1166 rows.\n",
      "Loaded and cleaned data for swiss_alps/st_moritz: 1014 rows.\n",
      "Loaded and cleaned data for swiss_alps/verbier: 1014 rows.\n",
      "Loaded and cleaned data for swiss_alps/verbier: 1166 rows.\n",
      "Loaded and cleaned data for swiss_alps/zermatt: 1014 rows.\n",
      "Loaded and cleaned data for swiss_alps/zermatt: 1166 rows.\n"
     ]
    }
   ],
   "source": [
    "# Define the root directory\n",
    "raw_data_root = '../data/raw/cds'\n",
    "\n",
    "# Get list of all CSV files with dataset type\n",
    "csv_files = get_all_csv_files_with_metadata(raw_data_root)\n",
    "print(f\"Found {len(csv_files)} CSV files (Old: {sum(1 for f in csv_files if f['type']=='old')}, New: {sum(1 for f in csv_files if f['type']=='new')}).\")\n",
    "\n",
    "data_frames = {}\n",
    "\n",
    "for file_info in csv_files:\n",
    "    key, df = clean_and_filter_data(file_info)\n",
    "    if key and df is not None:\n",
    "        data_frames[key] = df\n",
    "        print(f\"Loaded and cleaned data for {key}: {df.shape[0]} rows.\")\n",
    "    else:\n",
    "        print(f\"Failed to process {file_info['file_path']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Filter data based on each resort's opening and closing dates\n",
    "\n",
    "Each resort operates during specific dates in the year. We'll filter the data to include only the dates when each resort is open.\n",
    "\n",
    "Here are the approximate opening and closing dates for each resort:\n",
    "\n",
    "- **French Alps:**\n",
    "  - **Chamonix:** Opens mid-December (`12-15`), closes mid-May (`05-15`)\n",
    "  - **Val d'Isère & Tignes:** Opens November 30 (`11-30`), closes May 5 (`05-05`)\n",
    "  - **Les Trois Vallées:** Opens December 7 (`12-07`), closes mid-April (`04-15`)\n",
    "  \n",
    "- **Austrian Alps:**\n",
    "  - **St. Anton:** Opens early December (`12-01`), closes late April (`04-30`)\n",
    "  - **Kitzbühel:** Opens mid-October (`10-15`), closes May (`05-01`)\n",
    "  - **Sölden:** Opens early November (`11-01`), closes early May (`05-01`)\n",
    "  \n",
    "- **Swiss Alps:**\n",
    "  - **Zermatt:** Opens mid-November (`11-15`), closes late April (`04-30`)\n",
    "  - **St. Moritz:** Opens late November (`11-25`), closes early May (`05-01`)\n",
    "  - **Verbier:** Opens early December (`12-01`), closes late April (`04-30`)\n",
    "  \n",
    "- **Italian Alps:**\n",
    "  - **Cortina d'Ampezzo:** Opens late November (`11-25`), closes early April (`04-05`)\n",
    "  - **Val Gardena:** Opens early December (`12-01`), closes mid-April (`04-15`)\n",
    "  - **Sestriere:** Opens early December (`12-01`), closes mid-April (`04-15`)\n",
    "  \n",
    "- **Slovenian Alps:**\n",
    "  - **Kranjska Gora:** Opens mid-December (`12-15`), closes mid-April (`04-15`)\n",
    "  - **Mariborsko Pohorje:** Opens December (`12-01`), closes early April (`04-05`)\n",
    "  - **Krvavec:** Opens December (`12-01`), closes April (`04-30`)\n",
    "\n",
    "  We'll define the `resort_seasons` dictionary with normalized keys to match the keys in `data_frames`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resort_seasons = {\n",
    "    'french_alps/chamonix': {'open': '12-15', 'close': '05-15'},\n",
    "    'french_alps/val_d_isere_tignes': {'open': '11-30', 'close': '05-05'},\n",
    "    'french_alps/les_trois_vallees': {'open': '12-07', 'close': '04-15'},\n",
    "    'austrian_alps/st_anton': {'open': '12-01', 'close': '04-30'},\n",
    "    'austrian_alps/kitzbuhel': {'open': '10-15', 'close': '05-01'},\n",
    "    'austrian_alps/solden': {'open': '11-01', 'close': '05-01'},\n",
    "    'swiss_alps/zermatt': {'open': '11-15', 'close': '04-30'},\n",
    "    'swiss_alps/st_moritz': {'open': '11-25', 'close': '05-01'},\n",
    "    'swiss_alps/verbier': {'open': '12-01', 'close': '04-30'},\n",
    "    'italian_alps/cortina_d_ampezzo': {'open': '11-25', 'close': '04-05'},\n",
    "    'italian_alps/val_gardena': {'open': '12-01', 'close': '04-15'},\n",
    "    'italian_alps/sestriere': {'open': '12-01', 'close': '04-15'},\n",
    "    'slovenian_alps/kranjska_gora': {'open': '12-15', 'close': '04-15'},\n",
    "    'slovenian_alps/mariborsko_pohorje': {'open': '12-01', 'close': '04-05'},\n",
    "    'slovenian_alps/krvavec': {'open': '12-01', 'close': '04-30'},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Handles seasons that span across years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_dates(year, open_mm_dd, close_mm_dd):\n",
    "    \"\"\"\n",
    "    Given a year and open/close month-day strings, return datetime objects for open and close dates.\n",
    "    Handles seasons that span across years.\n",
    "    \"\"\"\n",
    "    open_month, open_day = map(int, open_mm_dd.split('-'))\n",
    "    close_month, close_day = map(int, close_mm_dd.split('-'))\n",
    "    \n",
    "    open_date = pd.Timestamp(year=year, month=open_month, day=open_day)\n",
    "    close_date = pd.Timestamp(year=year, month=close_month, day=close_day)\n",
    "    \n",
    "    # If close_date is earlier than open_date, it spans to the next year\n",
    "    if close_date < open_date:\n",
    "        close_date += pd.DateOffset(years=1)\n",
    "    \n",
    "    return open_date, close_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Assign rows to dataframe to a season based on the operating dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_season(df, season_info, resort_key):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing 'date' column.\n",
    "    - season_info (dict): Dictionary with 'open' and 'close' dates in 'MM-DD' format.\n",
    "    - resort_key (str): Key to identify the resort (e.g., 'french_alps/chamonix').\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with an added 'season_id' column.\n",
    "    \"\"\"\n",
    "    if not season_info:\n",
    "        # No season information provided\n",
    "        df['season_id'] = None\n",
    "        return df\n",
    "    \n",
    "    open_mm_dd = season_info['open']\n",
    "    close_mm_dd = season_info['close']\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['season_id'] = None  # Initialize season identifier\n",
    "    \n",
    "    years = df['date'].dt.year.unique()\n",
    "    \n",
    "    for year in years:\n",
    "        open_date, close_date = get_season_dates(year, open_mm_dd, close_mm_dd)\n",
    "        \n",
    "        # Filter rows within the current season\n",
    "        season_mask = (df['date'] >= open_date) & (df['date'] <= close_date)\n",
    "        season_label = f\"{year}-{close_date.year}\"\n",
    "        \n",
    "        df.loc[season_mask, 'season_id'] = season_label\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Apply Season Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in data_frames.items():\n",
    "    resort = key\n",
    "    if resort in resort_seasons:\n",
    "        season_info = resort_seasons[resort]\n",
    "        \n",
    "        # Categorize seasons\n",
    "        df = categorize_season(df, season_info, resort)\n",
    "        \n",
    "        # Update the DataFrame in the dictionary\n",
    "        data_frames[key] = df\n",
    "        print(f\"Season categorized for {resort}.\")\n",
    "    else:\n",
    "        print(f\"No season information for {resort}. Data not categorized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Add Operating Season Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_operating_season_indicator(df):\n",
    "    \"\"\"\n",
    "    Adds a boolean column 'is_operating_season' indicating if the row is within an operating season.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['is_operating_season'] = df['season_id'].notnull()\n",
    "    return df\n",
    "\n",
    "for key, df in data_frames.items():\n",
    "    df = add_operating_season_indicator(df)\n",
    "    data_frames[key] = df\n",
    "    print(f\"Operating season indicator added for {key}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Handle snow_depth and snowfall_sum Separately\n",
    "\n",
    "Since snow_depth and snowfall_sum are distinct metrics, handle them based on dataset type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in data_frames.items():\n",
    "    dataset_type = 'new' if 'snow_depth' in df.columns else 'old'\n",
    "    \n",
    "    if dataset_type == 'new':\n",
    "        # Handle 'snow_depth'\n",
    "        # Set 'snow_depth' to zero outside operating seasons\n",
    "        df.loc[~df['is_operating_season'], 'snow_depth'] = 0\n",
    "        \n",
    "        # Impute missing 'snow_depth' within operating seasons\n",
    "        # Example: Mean Imputation\n",
    "        mean_snow_depth = df[df['is_operating_season']]['snow_depth'].mean()\n",
    "        df.loc[df['is_operating_season'] & df['snow_depth'].isnull(), 'snow_depth_imputed'] = mean_snow_depth\n",
    "        \n",
    "    elif dataset_type == 'old':\n",
    "        # Handle 'snowfall_sum'\n",
    "        # Set 'snowfall_sum' to zero outside operating seasons\n",
    "        df.loc[~df['is_operating_season'], 'snowfall_sum'] = 0\n",
    "        \n",
    "        # Impute missing 'snowfall_sum' within operating seasons\n",
    "        # Example: Mean Imputation\n",
    "        mean_snowfall_sum = df[df['is_operating_season']]['snowfall_sum'].mean()\n",
    "        df.loc[df['is_operating_season'] & df['snowfall_sum'].isnull(), 'snowfall_sum_imputed'] = mean_snowfall_sum\n",
    "    \n",
    "    data_frames[key] = df\n",
    "    print(f\"Handled snow metrics for {key}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10  Impute Missing Values Appropriately\n",
    "\n",
    "Implement imputation methods based on dataset type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4/5 Filter Data Based on Resort Operating Dates\n",
    "\n",
    "We'll filter each resort's data to include only dates within its operating season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in data_frames.items():\n",
    "    resort = key\n",
    "    if resort in resort_seasons:\n",
    "        season = resort_seasons[resort]\n",
    "        open_month_day = season['open']\n",
    "        close_month_day = season['close']\n",
    "        \n",
    "        # Since the data spans multiple years, we need to filter for each year\n",
    "        df['year'] = df['date'].dt.year\n",
    "        filtered_dfs = []\n",
    "        \n",
    "        for year in df['year'].unique():\n",
    "            open_date_str = f\"{year}-{open_month_day}\"\n",
    "            close_date_str = f\"{year}-{close_month_day}\"\n",
    "            open_date = pd.to_datetime(open_date_str, errors='coerce').tz_localize(None)\n",
    "            close_date = pd.to_datetime(close_date_str, errors='coerce').tz_localize(None)\n",
    "            \n",
    "            # Handle cases where the season spans over the new year\n",
    "            if close_date < open_date:\n",
    "                # Season spans over to the next year\n",
    "                close_date += relativedelta(years=1)\n",
    "            \n",
    "            season_df = df[(df['date'] >= open_date) & (df['date'] <= close_date)]\n",
    "            filtered_dfs.append(season_df)\n",
    "        \n",
    "        # Combine all seasons\n",
    "        df_season = pd.concat(filtered_dfs)\n",
    "        \n",
    "        # Drop the 'year' column\n",
    "        df_season = df_season.drop(columns=['year'])\n",
    "        \n",
    "        # Update the DataFrame in the dictionary\n",
    "        data_frames[key] = df_season.reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Filtered data for {resort}: {df_season.shape[0]} rows within operating dates.\")\n",
    "    else:\n",
    "        print(f\"No season information for {resort}. Data not filtered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Cleaned Data\n",
    "\n",
    "We'll save the cleaned and filtered DataFrames to the `data/processed` directory, maintaining the normalized folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in data_frames.items():\n",
    "    # Split the key back into country and resort\n",
    "    country, resort = key.split('/')\n",
    "    # Build the processed data path\n",
    "    processed_dir = os.path.join(processed_data_root, country, resort)\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    # Save the cleaned DataFrame\n",
    "    processed_file_path = os.path.join(processed_dir, f\"{resort}_cleaned.csv\")\n",
    "    df.to_csv(processed_file_path, index=False)\n",
    "    print(f\"Saved cleaned data to {processed_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "- Loaded and cleaned data for all resorts.\n",
    "- Normalized resort names to handle special characters.\n",
    "- Filtered data based on each resort's operating dates.\n",
    "- Saved cleaned data to the `data/processed` directory.\n",
    "\n",
    "These datasets are now ready for feature engineering and further analysis.\n",
    "\n",
    "We have further data to analyseand clean.  Furthermore, data impudation may be required following aforemtnioend analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
